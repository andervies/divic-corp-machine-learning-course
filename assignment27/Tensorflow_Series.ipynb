{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1JnFRTJ_tl40lSdlsqddxvYSd3HU7Tj5_",
      "authorship_tag": "ABX9TyP0X4k/vqM2wf1UA2NJIoE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andervies/divic-corp-machine-learning-course/blob/main/assignment27/Tensorflow_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem One: Looking back on scratch"
      ],
      "metadata": {
        "id": "cSr1mA4jpq3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Initialize the Weights:**\n",
        "    - I used different initializers like Xavier, He, and SimpleInitializer to initialize the weights and biases for each layer.\n",
        "\n",
        "2. **Epoch Loop:**\n",
        "    - I implemented a loop to run the training process for a specified number of epochs.\n",
        "\n",
        "3. **Batch Processing:**\n",
        "    - I created mini-batches from the training data to process the data in smaller chunks.\n",
        "\n",
        "4. **Forward Propagation:**\n",
        "    - I implemented the forward pass for each layer type, including Convolutional, MaxPooling, Flatten, and Fully Connected layers.\n",
        "\n",
        "5. **Activation Functions:**\n",
        "    - I used activation functions like ReLU, Sigmoid, and Tanh in the forward and backward passes.\n",
        "\n",
        "6. **Loss Calculation:**\n",
        "    - I calculated the loss using functions like Softmax with Cross-Entropy Loss.\n",
        "\n",
        "7. **Backward Propagation:**\n",
        "    - I implemented the backward pass to compute gradients for each layer.\n",
        "\n",
        "8. **Weight Update:**\n",
        "    - I updated the weights and biases using optimization algorithms like SGD and AdaGrad.\n",
        "\n",
        "9. **Data Preprocessing:**\n",
        "    - I loaded, normalized, and reshaped the dataset. I also performed one-hot encoding of the labels.\n",
        "\n",
        "10. **Model Evaluation:**\n",
        "    - I implemented functions to predict and evaluate the model's performance using metrics like accuracy."
      ],
      "metadata": {
        "id": "pyd58NuAlnq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Two: Consider the correspondence between scratch and TensorFlow"
      ],
      "metadata": {
        "id": "wCFFyMVAp2Pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Load and Prepare Data:**\n",
        "    - **Scratch:** Load dataset, preprocess, and split into training, validation, and test sets. Implement mini-batch processing.\n",
        "    - **TensorFlow:** Load dataset, preprocess, and split into training, validation, and test sets. Implement mini-batch processing using an iterator class.\n",
        "\n",
        "2. **Define Hyperparameters:**\n",
        "    - **Scratch:** Set hyperparameters such as learning rate, batch size, number of epochs, and number of nodes in each layer.\n",
        "    - **TensorFlow:** Define hyperparameters like learning rate, batch size, number of epochs, and number of nodes in each layer using variables.\n",
        "\n",
        "3. **Initialize Weights and Biases:**\n",
        "    - **Scratch:** Manually initialize weights and biases for each layer using custom functions or classes.\n",
        "    - **TensorFlow:** Use TensorFlow’s built-in methods to initialize weights and biases, such as `tf.Variable` and `tf.random_normal`.\n",
        "\n",
        "4. **Build the Model:**\n",
        "    - **Scratch:** Manually implement forward propagation through each layer, applying activation functions.\n",
        "    - **TensorFlow:** Define the model architecture using TensorFlow operations like `tf.add`, `tf.matmul`, and activation functions such as `tf.nn.relu`.\n",
        "\n",
        "5. **Define Loss and Optimizer:**\n",
        "    - **Scratch:** Implement loss function and manually compute gradients for backpropagation.\n",
        "    - **TensorFlow:** Use TensorFlow’s built-in loss functions (e.g., `tf.nn.sigmoid_cross_entropy_with_logits`) and optimizers (e.g., `tf.train.AdamOptimizer`).\n",
        "\n",
        "6. **Training Loop:**\n",
        "    - **Scratch:** Write a loop to iterate over epochs and mini-batches, updating weights using computed gradients.\n",
        "    - **TensorFlow:** Use TensorFlow’s session to run the computation graph, executing the training operation for each mini-batch and epoch.\n",
        "\n",
        "7. **Evaluate Model:**\n",
        "    - **Scratch:** Manually compute accuracy and other metrics by comparing predictions with actual labels.\n",
        "    - **TensorFlow:** Use TensorFlow operations to compute accuracy and other metrics, such as `tf.reduce_mean` and `tf.cast`.\n",
        "\n",
        "**Summary:**\n",
        "- TensorFlow automates many steps involved in implementing deep learning models, such as weight initialization, forward propagation, loss calculation, and backpropagation.\n",
        "- It provides a structured and efficient way to build, train, and evaluate models using its computation graph and built-in functions.\n",
        "- This abstraction allows for easier experimentation and scaling of models compared to manual implementation from scratch."
      ],
      "metadata": {
        "id": "sA9xbdMUozBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = tf.compat.v1\n",
        "tf.disable_eager_execution()"
      ],
      "metadata": {
        "id": "jMqixhmPSlrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbadYWlSG5VN",
        "outputId": "e7df545d-8973-4b3a-ac84-a83f0bbf18a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 7.0241, val_loss : 67.6860, acc : 0.375\n",
            "Epoch 1, loss : 3.4241, val_loss : 23.4026, acc : 0.312\n",
            "Epoch 2, loss : 1.9387, val_loss : 11.6681, acc : 0.375\n",
            "Epoch 3, loss : 2.0917, val_loss : 13.1400, acc : 0.312\n",
            "Epoch 4, loss : 1.7685, val_loss : 17.7284, acc : 0.312\n",
            "Epoch 5, loss : 1.6097, val_loss : 12.9607, acc : 0.312\n",
            "Epoch 6, loss : 1.4402, val_loss : 10.0593, acc : 0.312\n",
            "Epoch 7, loss : 1.3704, val_loss : 9.4797, acc : 0.312\n",
            "Epoch 8, loss : 1.2536, val_loss : 9.8518, acc : 0.312\n",
            "Epoch 9, loss : 1.1476, val_loss : 8.5670, acc : 0.375\n",
            "Epoch 10, loss : 1.0930, val_loss : 8.0430, acc : 0.375\n",
            "Epoch 11, loss : 1.0412, val_loss : 7.8791, acc : 0.375\n",
            "Epoch 12, loss : 0.9804, val_loss : 7.1233, acc : 0.375\n",
            "Epoch 13, loss : 0.9326, val_loss : 6.7908, acc : 0.375\n",
            "Epoch 14, loss : 0.8792, val_loss : 6.2492, acc : 0.375\n",
            "Epoch 15, loss : 0.8304, val_loss : 5.7680, acc : 0.375\n",
            "Epoch 16, loss : 0.7835, val_loss : 5.2886, acc : 0.438\n",
            "Epoch 17, loss : 0.7384, val_loss : 4.8037, acc : 0.438\n",
            "Epoch 18, loss : 0.6961, val_loss : 4.3575, acc : 0.500\n",
            "Epoch 19, loss : 0.6543, val_loss : 3.9175, acc : 0.500\n",
            "Epoch 20, loss : 0.6136, val_loss : 3.5188, acc : 0.500\n",
            "Epoch 21, loss : 0.5738, val_loss : 3.1371, acc : 0.500\n",
            "Epoch 22, loss : 0.5349, val_loss : 2.7697, acc : 0.500\n",
            "Epoch 23, loss : 0.4973, val_loss : 2.4528, acc : 0.562\n",
            "Epoch 24, loss : 0.4606, val_loss : 2.1728, acc : 0.562\n",
            "Epoch 25, loss : 0.4255, val_loss : 1.9324, acc : 0.625\n",
            "Epoch 26, loss : 0.3919, val_loss : 1.7049, acc : 0.625\n",
            "Epoch 27, loss : 0.3609, val_loss : 1.5248, acc : 0.688\n",
            "Epoch 28, loss : 0.3326, val_loss : 1.3724, acc : 0.750\n",
            "Epoch 29, loss : 0.3071, val_loss : 1.2386, acc : 0.750\n",
            "Epoch 30, loss : 0.2851, val_loss : 1.1454, acc : 0.750\n",
            "Epoch 31, loss : 0.2647, val_loss : 1.0364, acc : 0.750\n",
            "Epoch 32, loss : 0.2466, val_loss : 0.9368, acc : 0.750\n",
            "Epoch 33, loss : 0.2297, val_loss : 0.8304, acc : 0.750\n",
            "Epoch 34, loss : 0.2155, val_loss : 0.7243, acc : 0.750\n",
            "Epoch 35, loss : 0.2024, val_loss : 0.6215, acc : 0.750\n",
            "Epoch 36, loss : 0.1920, val_loss : 0.5405, acc : 0.812\n",
            "Epoch 37, loss : 0.1815, val_loss : 0.4515, acc : 0.812\n",
            "Epoch 38, loss : 0.1735, val_loss : 0.4016, acc : 0.812\n",
            "Epoch 39, loss : 0.1643, val_loss : 0.3323, acc : 0.812\n",
            "Epoch 40, loss : 0.1574, val_loss : 0.2935, acc : 0.875\n",
            "Epoch 41, loss : 0.1496, val_loss : 0.2519, acc : 0.875\n",
            "Epoch 42, loss : 0.1429, val_loss : 0.2206, acc : 0.875\n",
            "Epoch 43, loss : 0.1364, val_loss : 0.1932, acc : 0.875\n",
            "Epoch 44, loss : 0.1302, val_loss : 0.1684, acc : 0.875\n",
            "Epoch 45, loss : 0.1244, val_loss : 0.1464, acc : 0.875\n",
            "Epoch 46, loss : 0.1188, val_loss : 0.1264, acc : 0.875\n",
            "Epoch 47, loss : 0.1139, val_loss : 0.1097, acc : 0.875\n",
            "Epoch 48, loss : 0.1091, val_loss : 0.0947, acc : 0.938\n",
            "Epoch 49, loss : 0.1050, val_loss : 0.0833, acc : 0.938\n",
            "Epoch 50, loss : 0.1007, val_loss : 0.0712, acc : 1.000\n",
            "Epoch 51, loss : 0.0975, val_loss : 0.0653, acc : 1.000\n",
            "Epoch 52, loss : 0.0932, val_loss : 0.0531, acc : 1.000\n",
            "Epoch 53, loss : 0.0915, val_loss : 0.0557, acc : 1.000\n",
            "Epoch 54, loss : 0.0862, val_loss : 0.0390, acc : 1.000\n",
            "Epoch 55, loss : 0.0875, val_loss : 0.0598, acc : 0.938\n",
            "Epoch 56, loss : 0.0790, val_loss : 0.0376, acc : 1.000\n",
            "Epoch 57, loss : 0.0871, val_loss : 0.0935, acc : 0.938\n",
            "Epoch 58, loss : 0.0723, val_loss : 0.0861, acc : 0.938\n",
            "Epoch 59, loss : 0.0936, val_loss : 0.1730, acc : 0.938\n",
            "Epoch 60, loss : 0.0693, val_loss : 0.1648, acc : 0.938\n",
            "Epoch 61, loss : 0.1047, val_loss : 0.2713, acc : 0.938\n",
            "Epoch 62, loss : 0.0711, val_loss : 0.1687, acc : 0.938\n",
            "Epoch 63, loss : 0.1072, val_loss : 0.3121, acc : 0.938\n",
            "Epoch 64, loss : 0.0738, val_loss : 0.1538, acc : 0.938\n",
            "Epoch 65, loss : 0.1069, val_loss : 0.3194, acc : 0.938\n",
            "Epoch 66, loss : 0.0757, val_loss : 0.1525, acc : 0.938\n",
            "Epoch 67, loss : 0.1082, val_loss : 0.3200, acc : 0.938\n",
            "Epoch 68, loss : 0.0780, val_loss : 0.1584, acc : 0.938\n",
            "Epoch 69, loss : 0.1114, val_loss : 0.3153, acc : 0.938\n",
            "Epoch 70, loss : 0.0808, val_loss : 0.1600, acc : 0.938\n",
            "Epoch 71, loss : 0.1139, val_loss : 0.2888, acc : 0.938\n",
            "Epoch 72, loss : 0.0824, val_loss : 0.1424, acc : 0.938\n",
            "Epoch 73, loss : 0.1110, val_loss : 0.2301, acc : 0.938\n",
            "Epoch 74, loss : 0.0791, val_loss : 0.1062, acc : 0.938\n",
            "Epoch 75, loss : 0.1002, val_loss : 0.1769, acc : 0.938\n",
            "Epoch 76, loss : 0.0719, val_loss : 0.0579, acc : 0.938\n",
            "Epoch 77, loss : 0.0810, val_loss : 0.0948, acc : 0.938\n",
            "Epoch 78, loss : 0.0610, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 79, loss : 0.0601, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 80, loss : 0.0514, val_loss : 0.0117, acc : 1.000\n",
            "Epoch 81, loss : 0.0498, val_loss : 0.0060, acc : 1.000\n",
            "Epoch 82, loss : 0.0454, val_loss : 0.0173, acc : 1.000\n",
            "Epoch 83, loss : 0.0472, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 84, loss : 0.0430, val_loss : 0.0289, acc : 1.000\n",
            "Epoch 85, loss : 0.0449, val_loss : 0.0094, acc : 1.000\n",
            "Epoch 86, loss : 0.0400, val_loss : 0.0493, acc : 0.938\n",
            "Epoch 87, loss : 0.0430, val_loss : 0.0089, acc : 1.000\n",
            "Epoch 88, loss : 0.0374, val_loss : 0.0669, acc : 0.938\n",
            "Epoch 89, loss : 0.0429, val_loss : 0.0068, acc : 1.000\n",
            "Epoch 90, loss : 0.0361, val_loss : 0.0709, acc : 0.938\n",
            "Epoch 91, loss : 0.0422, val_loss : 0.0058, acc : 1.000\n",
            "Epoch 92, loss : 0.0349, val_loss : 0.0692, acc : 0.938\n",
            "Epoch 93, loss : 0.0411, val_loss : 0.0055, acc : 1.000\n",
            "Epoch 94, loss : 0.0337, val_loss : 0.0678, acc : 0.938\n",
            "Epoch 95, loss : 0.0397, val_loss : 0.0059, acc : 1.000\n",
            "Epoch 96, loss : 0.0326, val_loss : 0.0613, acc : 0.938\n",
            "Epoch 97, loss : 0.0376, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 98, loss : 0.0316, val_loss : 0.0499, acc : 0.938\n",
            "Epoch 99, loss : 0.0349, val_loss : 0.0138, acc : 1.000\n",
            "test_acc : 0.900\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Binary classification of Iris dataset using neural network implemented in TensorFlow\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "tf = tf.compat.v1\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "tf.test.gpu_device_name()\n",
        "\"\"\"\n",
        "Don't forget when you change the version of tensorflow to the 1.x series.\n",
        "Install the GPU with \"!pip install tensorflow-gpu==1.14.0\".\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "If successful, a log is output; if not recognized, nothing is output。\n",
        "\"\"\"\n",
        "\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"drive/MyDrive/assignment27/Iris.csv\")\n",
        "\n",
        "#Condition extraction from data frame\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# Convert to NumPy array\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "#Convert labels to numbers\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    #Declaration of weights and biases\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "#Read network structure\n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "#Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Three: Create a model of Iris using all three types of objective variables"
      ],
      "metadata": {
        "id": "gCwQ6bm3QD02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# Filter data for three-class classification\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# Convert to NumPy array\n",
        "X = np.array(X)\n",
        "y = np.array(pd.get_dummies(y))\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : ndarray, shape (n_samples, n_classes)\n",
        "      Correct answer values\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_classes = y_train.shape[1]\n",
        "\n",
        "# Define placeholders\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def neural_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weights and biases\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    output_layer = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])  # tf.add and + are equivalent\n",
        "    return output_layer\n",
        "\n",
        "# Read network structure\n",
        "logits = neural_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "\n",
        "# Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialization of variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0] / batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(f\"Epoch {epoch}, loss: {total_loss:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.3f}\")\n",
        "\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(f\"Test accuracy: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYrn5XuFK6IC",
        "outputId": "d8291e02-5fdd-433f-f2d8-1a9800540ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss: 20.5204, val_loss: 135.0502, val_acc: 0.333\n",
            "Epoch 1, loss: 15.0845, val_loss: 98.2559, val_acc: 0.333\n",
            "Epoch 2, loss: 10.3703, val_loss: 61.5122, val_acc: 0.533\n",
            "Epoch 3, loss: 6.0867, val_loss: 34.2166, val_acc: 0.267\n",
            "Epoch 4, loss: 2.8178, val_loss: 10.5846, val_acc: 0.533\n",
            "Epoch 5, loss: 0.5077, val_loss: 1.2543, val_acc: 0.867\n",
            "Epoch 6, loss: 0.0851, val_loss: 0.1723, val_acc: 0.933\n",
            "Epoch 7, loss: 0.0546, val_loss: 0.1215, val_acc: 0.933\n",
            "Epoch 8, loss: 0.0524, val_loss: 0.0879, val_acc: 0.933\n",
            "Epoch 9, loss: 0.0507, val_loss: 0.0755, val_acc: 0.933\n",
            "Epoch 10, loss: 0.0496, val_loss: 0.0678, val_acc: 0.933\n",
            "Epoch 11, loss: 0.0487, val_loss: 0.0610, val_acc: 0.933\n",
            "Epoch 12, loss: 0.0478, val_loss: 0.0545, val_acc: 0.933\n",
            "Epoch 13, loss: 0.0469, val_loss: 0.0487, val_acc: 0.933\n",
            "Epoch 14, loss: 0.0463, val_loss: 0.0435, val_acc: 1.000\n",
            "Epoch 15, loss: 0.0455, val_loss: 0.0375, val_acc: 1.000\n",
            "Epoch 16, loss: 0.0446, val_loss: 0.0337, val_acc: 1.000\n",
            "Epoch 17, loss: 0.0436, val_loss: 0.0334, val_acc: 1.000\n",
            "Epoch 18, loss: 0.0420, val_loss: 0.0311, val_acc: 1.000\n",
            "Epoch 19, loss: 0.0409, val_loss: 0.0297, val_acc: 1.000\n",
            "Epoch 20, loss: 0.0400, val_loss: 0.0268, val_acc: 1.000\n",
            "Epoch 21, loss: 0.0392, val_loss: 0.0227, val_acc: 1.000\n",
            "Epoch 22, loss: 0.0383, val_loss: 0.0203, val_acc: 1.000\n",
            "Epoch 23, loss: 0.0375, val_loss: 0.0194, val_acc: 1.000\n",
            "Epoch 24, loss: 0.0364, val_loss: 0.0173, val_acc: 1.000\n",
            "Epoch 25, loss: 0.0358, val_loss: 0.0158, val_acc: 1.000\n",
            "Epoch 26, loss: 0.0349, val_loss: 0.0132, val_acc: 1.000\n",
            "Epoch 27, loss: 0.0344, val_loss: 0.0117, val_acc: 1.000\n",
            "Epoch 28, loss: 0.0337, val_loss: 0.0103, val_acc: 1.000\n",
            "Epoch 29, loss: 0.0332, val_loss: 0.0086, val_acc: 1.000\n",
            "Epoch 30, loss: 0.0328, val_loss: 0.0074, val_acc: 1.000\n",
            "Epoch 31, loss: 0.0325, val_loss: 0.0061, val_acc: 1.000\n",
            "Epoch 32, loss: 0.0323, val_loss: 0.0053, val_acc: 1.000\n",
            "Epoch 33, loss: 0.0323, val_loss: 0.0049, val_acc: 1.000\n",
            "Epoch 34, loss: 0.0320, val_loss: 0.0054, val_acc: 1.000\n",
            "Epoch 35, loss: 0.0312, val_loss: 0.0050, val_acc: 1.000\n",
            "Epoch 36, loss: 0.0308, val_loss: 0.0044, val_acc: 1.000\n",
            "Epoch 37, loss: 0.0306, val_loss: 0.0042, val_acc: 1.000\n",
            "Epoch 38, loss: 0.0303, val_loss: 0.0045, val_acc: 1.000\n",
            "Epoch 39, loss: 0.0295, val_loss: 0.0047, val_acc: 1.000\n",
            "Epoch 40, loss: 0.0286, val_loss: 0.0049, val_acc: 1.000\n",
            "Epoch 41, loss: 0.0277, val_loss: 0.0049, val_acc: 1.000\n",
            "Epoch 42, loss: 0.0270, val_loss: 0.0047, val_acc: 1.000\n",
            "Epoch 43, loss: 0.0264, val_loss: 0.0044, val_acc: 1.000\n",
            "Epoch 44, loss: 0.0260, val_loss: 0.0042, val_acc: 1.000\n",
            "Epoch 45, loss: 0.0256, val_loss: 0.0039, val_acc: 1.000\n",
            "Epoch 46, loss: 0.0252, val_loss: 0.0037, val_acc: 1.000\n",
            "Epoch 47, loss: 0.0249, val_loss: 0.0035, val_acc: 1.000\n",
            "Epoch 48, loss: 0.0246, val_loss: 0.0033, val_acc: 1.000\n",
            "Epoch 49, loss: 0.0243, val_loss: 0.0032, val_acc: 1.000\n",
            "Epoch 50, loss: 0.0240, val_loss: 0.0030, val_acc: 1.000\n",
            "Epoch 51, loss: 0.0237, val_loss: 0.0029, val_acc: 1.000\n",
            "Epoch 52, loss: 0.0234, val_loss: 0.0028, val_acc: 1.000\n",
            "Epoch 53, loss: 0.0231, val_loss: 0.0027, val_acc: 1.000\n",
            "Epoch 54, loss: 0.0228, val_loss: 0.0026, val_acc: 1.000\n",
            "Epoch 55, loss: 0.0225, val_loss: 0.0025, val_acc: 1.000\n",
            "Epoch 56, loss: 0.0222, val_loss: 0.0024, val_acc: 1.000\n",
            "Epoch 57, loss: 0.0220, val_loss: 0.0024, val_acc: 1.000\n",
            "Epoch 58, loss: 0.0218, val_loss: 0.0023, val_acc: 1.000\n",
            "Epoch 59, loss: 0.0215, val_loss: 0.0023, val_acc: 1.000\n",
            "Epoch 60, loss: 0.0213, val_loss: 0.0022, val_acc: 1.000\n",
            "Epoch 61, loss: 0.0210, val_loss: 0.0022, val_acc: 1.000\n",
            "Epoch 62, loss: 0.0208, val_loss: 0.0022, val_acc: 1.000\n",
            "Epoch 63, loss: 0.0206, val_loss: 0.0022, val_acc: 1.000\n",
            "Epoch 64, loss: 0.0204, val_loss: 0.0024, val_acc: 1.000\n",
            "Epoch 65, loss: 0.0202, val_loss: 0.0023, val_acc: 1.000\n",
            "Epoch 66, loss: 0.0201, val_loss: 0.0023, val_acc: 1.000\n",
            "Epoch 67, loss: 0.0199, val_loss: 0.0023, val_acc: 1.000\n",
            "Epoch 68, loss: 0.0197, val_loss: 0.0023, val_acc: 1.000\n",
            "Epoch 69, loss: 0.0195, val_loss: 0.0024, val_acc: 1.000\n",
            "Epoch 70, loss: 0.0193, val_loss: 0.0024, val_acc: 1.000\n",
            "Epoch 71, loss: 0.0190, val_loss: 0.0022, val_acc: 1.000\n",
            "Epoch 72, loss: 0.0189, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 73, loss: 0.0189, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 74, loss: 0.0188, val_loss: 0.0022, val_acc: 1.000\n",
            "Epoch 75, loss: 0.0186, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 76, loss: 0.0185, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 77, loss: 0.0183, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 78, loss: 0.0182, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 79, loss: 0.0180, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 80, loss: 0.0179, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 81, loss: 0.0178, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 82, loss: 0.0177, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 83, loss: 0.0176, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 84, loss: 0.0174, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 85, loss: 0.0172, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 86, loss: 0.0172, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 87, loss: 0.0171, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 88, loss: 0.0169, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 89, loss: 0.0168, val_loss: 0.0019, val_acc: 1.000\n",
            "Epoch 90, loss: 0.0167, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 91, loss: 0.0166, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 92, loss: 0.0165, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 93, loss: 0.0164, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 94, loss: 0.0163, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 95, loss: 0.0162, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 96, loss: 0.0160, val_loss: 0.0020, val_acc: 1.000\n",
            "Epoch 97, loss: 0.0160, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 98, loss: 0.0158, val_loss: 0.0021, val_acc: 1.000\n",
            "Epoch 99, loss: 0.0158, val_loss: 0.0020, val_acc: 1.000\n",
            "Test accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Four: Create a model of House Prices"
      ],
      "metadata": {
        "id": "DJl8l51rPuXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "house_data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Select relevant columns\n",
        "X = house_data[[\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = house_data[\"SalePrice\"]\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)\n",
        "\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_output = 1\n",
        "\n",
        "# Define placeholders\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_output])\n",
        "\n",
        "# Train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def regression_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network for regression\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weights and biases\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_output]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_output]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    output_layer = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
        "    return output_layer\n",
        "\n",
        "# Read network structure\n",
        "logits = regression_net(X)\n",
        "\n",
        "# Objective function (Mean Squared Error)\n",
        "loss_op = tf.reduce_mean(tf.square(logits - Y))\n",
        "\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Initialization of variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0] / batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        print(f\"Epoch {epoch}, loss: {total_loss:.4f}, val_loss: {val_loss:.4f}\")\n",
        "\n",
        "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
        "    print(f\"Test loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOgkyUzVMepH",
        "outputId": "896f8198-8582-428a-9c4f-40d20748fd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss: 23732756448.0000, val_loss: 3691986944.0000\n",
            "Epoch 1, loss: 5639454295.5000, val_loss: 3748113920.0000\n",
            "Epoch 2, loss: 5462861265.5000, val_loss: 3841552384.0000\n",
            "Epoch 3, loss: 5396506829.5000, val_loss: 3910583296.0000\n",
            "Epoch 4, loss: 5381580262.5000, val_loss: 3942595840.0000\n",
            "Epoch 5, loss: 5376558308.5000, val_loss: 3966002944.0000\n",
            "Epoch 6, loss: 5373939226.0000, val_loss: 3979547136.0000\n",
            "Epoch 7, loss: 5372898390.0000, val_loss: 3984245504.0000\n",
            "Epoch 8, loss: 5372443442.0000, val_loss: 3982286848.0000\n",
            "Epoch 9, loss: 5372406419.0000, val_loss: 3985517056.0000\n",
            "Epoch 10, loss: 5372446631.0000, val_loss: 3982362368.0000\n",
            "Epoch 11, loss: 5372699777.5000, val_loss: 3982856960.0000\n",
            "Epoch 12, loss: 5373366258.5000, val_loss: 3980841984.0000\n",
            "Epoch 13, loss: 5373516686.0000, val_loss: 3978752512.0000\n",
            "Epoch 14, loss: 5374128545.5000, val_loss: 3976596224.0000\n",
            "Epoch 15, loss: 5374475719.5000, val_loss: 3973945600.0000\n",
            "Epoch 16, loss: 5374418526.5000, val_loss: 3971655424.0000\n",
            "Epoch 17, loss: 5374570385.0000, val_loss: 3967716352.0000\n",
            "Epoch 18, loss: 5375208154.0000, val_loss: 3967967232.0000\n",
            "Epoch 19, loss: 5375266081.5000, val_loss: 3961907968.0000\n",
            "Epoch 20, loss: 5375631009.0000, val_loss: 3958752768.0000\n",
            "Epoch 21, loss: 5376149280.0000, val_loss: 3955571712.0000\n",
            "Epoch 22, loss: 5376756576.5000, val_loss: 3952592384.0000\n",
            "Epoch 23, loss: 5375999655.0000, val_loss: 3947153408.0000\n",
            "Epoch 24, loss: 5375438851.0000, val_loss: 3941917184.0000\n",
            "Epoch 25, loss: 5374790624.5000, val_loss: 3936768256.0000\n",
            "Epoch 26, loss: 5374776377.5000, val_loss: 3932568832.0000\n",
            "Epoch 27, loss: 5374760656.5000, val_loss: 3931003648.0000\n",
            "Epoch 28, loss: 5375032756.0000, val_loss: 3928239872.0000\n",
            "Epoch 29, loss: 5374975922.5000, val_loss: 3925267456.0000\n",
            "Epoch 30, loss: 5374763740.5000, val_loss: 3923715840.0000\n",
            "Epoch 31, loss: 5375166505.5000, val_loss: 3919195648.0000\n",
            "Epoch 32, loss: 5374737332.5000, val_loss: 3914701056.0000\n",
            "Epoch 33, loss: 5375343044.0000, val_loss: 3914206720.0000\n",
            "Epoch 34, loss: 5374904869.5000, val_loss: 3912600320.0000\n",
            "Epoch 35, loss: 5375029069.0000, val_loss: 3910073088.0000\n",
            "Epoch 36, loss: 5374774575.0000, val_loss: 3907171072.0000\n",
            "Epoch 37, loss: 5375130837.5000, val_loss: 3905357568.0000\n",
            "Epoch 38, loss: 5374895484.0000, val_loss: 3902416128.0000\n",
            "Epoch 39, loss: 5374938368.5000, val_loss: 3902041600.0000\n",
            "Epoch 40, loss: 5374532688.5000, val_loss: 3900927488.0000\n",
            "Epoch 41, loss: 5374524803.0000, val_loss: 3897259776.0000\n",
            "Epoch 42, loss: 5374347271.0000, val_loss: 3894846720.0000\n",
            "Epoch 43, loss: 5373755270.0000, val_loss: 3892225792.0000\n",
            "Epoch 44, loss: 5374254808.5000, val_loss: 3890146816.0000\n",
            "Epoch 45, loss: 5374509461.0000, val_loss: 3889093888.0000\n",
            "Epoch 46, loss: 5374143712.5000, val_loss: 3886305792.0000\n",
            "Epoch 47, loss: 5373963906.5000, val_loss: 3883424512.0000\n",
            "Epoch 48, loss: 5373902375.5000, val_loss: 3882753024.0000\n",
            "Epoch 49, loss: 5373766417.0000, val_loss: 3881080576.0000\n",
            "Epoch 50, loss: 5373567045.0000, val_loss: 3879523840.0000\n",
            "Epoch 51, loss: 5373763910.0000, val_loss: 3877074176.0000\n",
            "Epoch 52, loss: 5373458633.5000, val_loss: 3876293632.0000\n",
            "Epoch 53, loss: 5373659436.0000, val_loss: 3876326400.0000\n",
            "Epoch 54, loss: 5373430382.0000, val_loss: 3874612736.0000\n",
            "Epoch 55, loss: 5373075269.0000, val_loss: 3870826496.0000\n",
            "Epoch 56, loss: 5373248310.5000, val_loss: 3871091200.0000\n",
            "Epoch 57, loss: 5373039484.5000, val_loss: 3867475456.0000\n",
            "Epoch 58, loss: 5372958001.5000, val_loss: 3867919872.0000\n",
            "Epoch 59, loss: 5372767131.5000, val_loss: 3864043776.0000\n",
            "Epoch 60, loss: 5372628754.0000, val_loss: 3863178496.0000\n",
            "Epoch 61, loss: 5372539836.5000, val_loss: 3860763136.0000\n",
            "Epoch 62, loss: 5372647387.5000, val_loss: 3860221184.0000\n",
            "Epoch 63, loss: 5372221688.0000, val_loss: 3856987136.0000\n",
            "Epoch 64, loss: 5372191319.0000, val_loss: 3856403456.0000\n",
            "Epoch 65, loss: 5372084405.5000, val_loss: 3853077760.0000\n",
            "Epoch 66, loss: 5371892482.0000, val_loss: 3852748288.0000\n",
            "Epoch 67, loss: 5371859976.5000, val_loss: 3852753152.0000\n",
            "Epoch 68, loss: 5371818944.5000, val_loss: 3850067712.0000\n",
            "Epoch 69, loss: 5371476311.5000, val_loss: 3848894720.0000\n",
            "Epoch 70, loss: 5371519451.0000, val_loss: 3846813696.0000\n",
            "Epoch 71, loss: 5371580498.0000, val_loss: 3845808640.0000\n",
            "Epoch 72, loss: 5371434981.5000, val_loss: 3843485696.0000\n",
            "Epoch 73, loss: 5371049791.0000, val_loss: 3841258496.0000\n",
            "Epoch 74, loss: 5371120570.0000, val_loss: 3839713792.0000\n",
            "Epoch 75, loss: 5371063639.5000, val_loss: 3838375168.0000\n",
            "Epoch 76, loss: 5375824747.5000, val_loss: 3841090048.0000\n",
            "Epoch 77, loss: 5370772724.5000, val_loss: 3837705984.0000\n",
            "Epoch 78, loss: 5370682166.0000, val_loss: 3834433792.0000\n",
            "Epoch 79, loss: 5370668954.5000, val_loss: 3833910272.0000\n",
            "Epoch 80, loss: 5370449912.0000, val_loss: 3831122688.0000\n",
            "Epoch 81, loss: 5370505952.0000, val_loss: 3829718272.0000\n",
            "Epoch 82, loss: 5370520630.0000, val_loss: 3827842304.0000\n",
            "Epoch 83, loss: 5369996178.0000, val_loss: 3825997568.0000\n",
            "Epoch 84, loss: 5369905787.5000, val_loss: 3822391808.0000\n",
            "Epoch 85, loss: 5369718070.5000, val_loss: 3823003392.0000\n",
            "Epoch 86, loss: 5369987337.0000, val_loss: 3822832384.0000\n",
            "Epoch 87, loss: 5369579889.0000, val_loss: 3819739648.0000\n",
            "Epoch 88, loss: 5369732166.5000, val_loss: 3819389696.0000\n",
            "Epoch 89, loss: 5369364591.5000, val_loss: 3818907136.0000\n",
            "Epoch 90, loss: 5369093195.0000, val_loss: 3815568640.0000\n",
            "Epoch 91, loss: 5369162051.0000, val_loss: 3815019776.0000\n",
            "Epoch 92, loss: 5369236825.5000, val_loss: 3814781952.0000\n",
            "Epoch 93, loss: 5368642356.0000, val_loss: 3811325184.0000\n",
            "Epoch 94, loss: 5368789080.0000, val_loss: 3809515776.0000\n",
            "Epoch 95, loss: 5368804092.0000, val_loss: 3810112768.0000\n",
            "Epoch 96, loss: 5368466532.0000, val_loss: 3807948544.0000\n",
            "Epoch 97, loss: 5368040109.5000, val_loss: 3784476416.0000\n",
            "Epoch 98, loss: 5369219975.0000, val_loss: 3787916544.0000\n",
            "Epoch 99, loss: 5368091164.5000, val_loss: 3788278528.0000\n",
            "Test loss: 3735785984.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Five: Create a model of MNIST"
      ],
      "metadata": {
        "id": "VHscahw0Pkmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "# Reshape and normalize pixel values\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Split into train, validation, and test sets (after reshaping)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "# One-hot encode labels (if needed for your model)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10) # One-hot encode validation labels\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "num_epochs = 10\n",
        "\n",
        "n_input = 28 * 28  # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "\n",
        "# Define placeholders\n",
        "X = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "\n",
        "# Train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def conv_net(x):\n",
        "    # Conv Layer 1\n",
        "    conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
        "    # Pooling Layer 1\n",
        "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
        "\n",
        "    # Conv Layer 2\n",
        "    conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
        "    # Pooling Layer 2\n",
        "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
        "\n",
        "    # Flatten tensor\n",
        "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
        "\n",
        "    # Dense Layer\n",
        "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
        "    # Dropout\n",
        "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=True)\n",
        "\n",
        "    # Logits Layer\n",
        "    logits = tf.layers.dense(inputs=dropout, units=n_classes)\n",
        "    return logits\n",
        "\n",
        "# Build the model\n",
        "logits = conv_net(X)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Train the model\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Reshape mini-batch to match placeholder shape\n",
        "            mini_batch_x = mini_batch_x.reshape(-1, 28, 28, 1)  # Reshape mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= len(get_mini_batch_train)\n",
        "        # Reshape validation data to match placeholder shape\n",
        "        X_val_reshaped = X_val.reshape(-1, 28, 28, 1)  # Reshape validation data\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val_reshaped, Y: y_val}) # Use reshaped validation data\n",
        "        print(f\"Epoch {epoch + 1}, loss: {total_loss:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.3f}\")\n",
        "\n",
        "    # Evaluate on test data\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(f\"Test Accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL7vL2tzdEwh",
        "outputId": "4dddae3e-51aa-4151-9dbb-8487fc0e7cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-a8b428b62df0>:40: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
            "<ipython-input-19-a8b428b62df0>:42: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
            "<ipython-input-19-a8b428b62df0>:45: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
            "<ipython-input-19-a8b428b62df0>:47: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
            "<ipython-input-19-a8b428b62df0>:53: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
            "<ipython-input-19-a8b428b62df0>:55: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=True)\n",
            "<ipython-input-19-a8b428b62df0>:58: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  logits = tf.layers.dense(inputs=dropout, units=n_classes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss: 0.1410, val_loss: 0.0577, val_acc: 0.982\n",
            "Epoch 2, loss: 0.0381, val_loss: 0.0492, val_acc: 0.986\n",
            "Epoch 3, loss: 0.0241, val_loss: 0.0460, val_acc: 0.986\n",
            "Epoch 4, loss: 0.0165, val_loss: 0.0368, val_acc: 0.988\n",
            "Epoch 5, loss: 0.0118, val_loss: 0.0380, val_acc: 0.990\n",
            "Epoch 6, loss: 0.0083, val_loss: 0.0415, val_acc: 0.990\n",
            "Epoch 7, loss: 0.0077, val_loss: 0.0359, val_acc: 0.991\n",
            "Epoch 8, loss: 0.0064, val_loss: 0.0573, val_acc: 0.987\n",
            "Epoch 9, loss: 0.0058, val_loss: 0.0428, val_acc: 0.989\n",
            "Epoch 10, loss: 0.0044, val_loss: 0.0421, val_acc: 0.990\n",
            "Test Accuracy: 0.992\n"
          ]
        }
      ]
    }
  ]
}