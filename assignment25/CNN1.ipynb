{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdbd9dc-87e5-4893-880a-afc53a17902e",
   "metadata": {
    "id": "dfdbd9dc-87e5-4893-880a-afc53a17902e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 22:57:01.327404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hKuCrgO6pTPR",
   "metadata": {
    "id": "hKuCrgO6pTPR"
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return self.sigmoid(A)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        _sig = self.sigmoid(self.A)\n",
    "        return dZ * (1 - _sig)*_sig\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "\n",
    "class Softmax:\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, Y):\n",
    "        self.loss = self.loss_func(Y)\n",
    "        return self.Z - Y\n",
    "\n",
    "    def loss_func(self, Y, Z=None):\n",
    "        if Z is None:\n",
    "            Z = self.Z\n",
    "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "\n",
    "class ReLU:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.clip(A, 0, None)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9e8619-2c6c-45a5-ab9a-38ddebfa0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        A = X@self.W + self.B\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = dA@self.W.T\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = self.X.T@dA\n",
    "        self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f238af3-030f-49df-810e-e5ecc87a93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = math.sqrt(1 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "\n",
    "class HeInitializer():\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = math.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "\n",
    "\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, *shape):\n",
    "        W = self.sigma * np.random.randn(*shape)\n",
    "        return W\n",
    "        \n",
    "    def B(self, *shape):\n",
    "        B = self.sigma * np.random.randn(*shape)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77bc645a-225c-45d6-89c0-ea4b1b9adba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return\n",
    "\n",
    "class AdaGrad:\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW = 1\n",
    "        self.HB = 1\n",
    "\n",
    "    def update(self, layer):\n",
    "        self.HW += layer.dW**2\n",
    "        self.HB += layer.dB**2\n",
    "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
    "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90090c9-e9e4-4d73-a9af-0a54396f21e5",
   "metadata": {
    "id": "f90090c9-e9e4-4d73-a9af-0a54396f21e5"
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71856d-743f-4d98-8e30-8a6681f0c82a",
   "metadata": {
    "id": "d3075aaf-3379-4b17-9429-9069f775fddd"
   },
   "source": [
    "## Problem One: Creating a one-dimensional convolutional layer class that limits the number of channels to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e0d7c8-4cf2-4e03-b256-9e18ee9a4ce3",
   "metadata": {
    "id": "07e0d7c8-4cf2-4e03-b256-9e18ee9a4ce3"
   },
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "    def forward(self, x, w, b):\n",
    "        a = []\n",
    "        for i in range(len(w) - 1):\n",
    "            a.append((x[i:i+len(w)] @ w) + b[0])\n",
    "        return np.array(a)\n",
    "    def backward(self, x, w, da):\n",
    "        db = np.sum(da)\n",
    "        dw = []\n",
    "        for i in range(len(w)):\n",
    "            dw.append(da @ x[i:i+len(da)])\n",
    "        dw = np.array(dw)\n",
    "        dx = []\n",
    "        new_w = np.insert(w[::-1], 0, 0)\n",
    "        new_w = np.append(new_w, 0)\n",
    "        for i in range(len(new_w)-1):\n",
    "            dx.append(new_w[i:i+len(da)] @ da)\n",
    "        dx = np.array(dx[::-1])\n",
    "        return db, dw, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c4e488-dfef-4941-b541-b571885fe88a",
   "metadata": {},
   "source": [
    "## Problem Two: Calculation of output size after 1D convolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a70db8-d42d-4008-b3d7-9ca941ff33c2",
   "metadata": {
    "id": "f1a70db8-d42d-4008-b3d7-9ca941ff33c2"
   },
   "outputs": [],
   "source": [
    "def output_size_calculation(n_in, F, P=0, S=1):\n",
    "    n_out = int((n_in + 2*P - F) / S + 1)\n",
    "\n",
    "    return n_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4dc6e-0a74-4250-928d-cf903928ca59",
   "metadata": {
    "id": "2e01687a-469d-42e4-b7f6-c6832c303699"
   },
   "source": [
    "## Problem Three: Experiments on 1D convolutional layers with small arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac16151-57c5-4464-ba3c-2025081b7265",
   "metadata": {
    "id": "4ac16151-57c5-4464-ba3c-2025081b7265"
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3c05af-8733-4b43-ba01-bec2e4c3910e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_conv_1d = SimpleConv1d()\n",
    "simple_conv_1d.forward(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f761277a-d09a-43fe-826c-10727f9924f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f761277a-d09a-43fe-826c-10727f9924f2",
    "outputId": "a00d755e-29db-425a-afae-381ac0448b02"
   },
   "outputs": [],
   "source": [
    "a = np.array([35, 50])\n",
    "a_actual = np.array([45, 70])\n",
    "da = np.array([10, 20])\n",
    "db, dw, dx = simple_conv_1d.backward(x, w, da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf51c7c-ba4f-44c0-9482-ca5300d3a3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 50]\n",
      "30\n",
      "[ 50  80 110]\n",
      "[ 30 110 170 140]\n"
     ]
    }
   ],
   "source": [
    "print(simple_conv_1d.forward(x, w,b))\n",
    "print(db)\n",
    "print(dw)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9517507-5e21-4b84-a7f5-52613f9f348c",
   "metadata": {
    "id": "4c877805-201e-465f-9248-d1fcf2fd25df"
   },
   "source": [
    "## Problem Four: Creation of 1D convolutional layer class without limiting the number of channels & Problem Five: (Advanced task) Implementing padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d5aa64-1794-43f0-8dea-7b154a3e717b",
   "metadata": {
    "id": "86d5aa64-1794-43f0-8dea-7b154a3e717b"
   },
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "\n",
    "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0):\n",
    "        self.b_size = b_size\n",
    "        self.optimizer = optimizer\n",
    "        self.pa = pa\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
    "        self.B = initializer.B(n_out_channels)\n",
    "        self.n_in_channels = n_in_channels\n",
    "        self.n_out_channels = n_out_channels\n",
    "        self.n_out = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.n_in = X.shape[-1]\n",
    "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa)\n",
    "        X = X.reshape(self.n_in_channels, self.n_in)\n",
    "        self.X = np.pad(X, ((0,0), ((self.b_size-1), 0)))\n",
    "        self.X1 = np.zeros((self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
    "        for i in range(self.b_size):\n",
    "            self.X1[:, i] = np.roll(self.X, -i, axis=-1)\n",
    "        A = np.sum(self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa]*self.W[:, :, :, np.newaxis], axis=(1, 2)) + self.B.reshape(-1,1)\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        self.dW = np.sum(np.dot(dA, self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa, np.newaxis]), axis=-1)\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        self.dA = np.pad(dA, ((0,0), (0, (self.b_size-1))))\n",
    "        self.dA1 = np.zeros((self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
    "        for i in range(self.b_size):\n",
    "            self.dA1[:, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W@self.dA1, axis=0)\n",
    "        self.optimizer.update(self)\n",
    "        print(\"dW:\", self.dW)\n",
    "        print(\"dB:\", self.dB)\n",
    "        print(\"dX:\", dX)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b812dd0-68b4-4a9a-9b26-4c7c718e8746",
   "metadata": {
    "id": "0b812dd0-68b4-4a9a-9b26-4c7c718e8746"
   },
   "outputs": [],
   "source": [
    "conv1d = Conv1d(b_size = 3, initializer =SimpleInitializer(0.01),optimizer=AdaGrad(0.01), n_in_channels=2, n_out_channels=3, pa=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af53ea5b-312e-431a-83fb-77a2f8ff8bc1",
   "metadata": {
    "id": "af53ea5b-312e-431a-83fb-77a2f8ff8bc1"
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3,4], [2,3,4,5]])\n",
    "conv1d.W =np.ones((3,2,3), dtype=float)\n",
    "conv1d.B = np.array([1,2,3], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "954dcd19-9c38-4821-976f-276294f3c7c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "954dcd19-9c38-4821-976f-276294f3c7c0",
    "outputId": "85abf5c2-e82b-458c-b63e-6cc27496fd63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_forward =conv1d.forward(X)\n",
    "conv1d_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4b33d19-07f7-4bd3-8e89-51a245c77fae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4b33d19-07f7-4bd3-8e89-51a245c77fae",
    "outputId": "9a5cb5d1-a8fb-4c1e-82e5-6fdf94f4cba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [[[ 60.  98. 136.]\n",
      "  [ 98. 136. 174.]]\n",
      "\n",
      " [[ 63. 103. 143.]\n",
      "  [103. 143. 183.]]\n",
      "\n",
      " [[ 66. 108. 150.]\n",
      "  [108. 150. 192.]]]\n",
      "dB: [38. 40. 42.]\n",
      "dX: [[ 51. 120. 120.  69.]\n",
      " [ 51. 120. 120.  69.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 51., 120., 120.,  69.],\n",
       "       [ 51., 120., 120.,  69.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.backward(conv1d_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e604866-424a-4f53-8a2f-ec83d781df3d",
   "metadata": {},
   "source": [
    "## Problem Seven: (Advance assignment) Arbitrary number of strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "PTFwtnNapKW-",
   "metadata": {
    "id": "PTFwtnNapKW-"
   },
   "outputs": [],
   "source": [
    "class Conv1d_Arbitrary_Strides:\n",
    "\n",
    "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride=1):\n",
    "        self.b_size = b_size\n",
    "        self.optimizer = optimizer\n",
    "        self.pa = pa\n",
    "        self.stride = stride\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
    "        self.B = initializer.B(n_out_channels)\n",
    "        self.n_in_channels = n_in_channels\n",
    "        self.n_out_channels = n_out_channels\n",
    "        self.n_out = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_in = X.shape[-1]\n",
    "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa, self.stride)\n",
    "        X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n",
    "        self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\n",
    "        self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
    "        for i in range(self.b_size):\n",
    "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
    "        A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n",
    "        self.dB = np.sum(dA, axis=(0, -1))\n",
    "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n",
    "        self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
    "        for i in range(self.b_size):\n",
    "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
    "        self.optimizer.update(self)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dpnSX3dpOKI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dpnSX3dpOKI",
    "outputId": "6a02cbbd-ecf0-49fe-e4c0-f3e5e4122df5"
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# Reshape and normalize the data\n",
    "X_train = X_train.reshape(-1, 784).astype(np.float64) / 255\n",
    "X_test = X_test.reshape(-1, 784).astype(np.float64) / 255\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encoding of the labels after splitting\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_val_one_hot = enc.transform(y_val[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64af5d-7048-4cad-865e-4ba5023612e7",
   "metadata": {},
   "source": [
    "## Problem Six: (Advanced task) Response to mini batch & Problem Eight: Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "qy-jeT3xpZdN",
   "metadata": {
    "id": "qy-jeT3xpZdN"
   },
   "outputs": [],
   "source": [
    "class ScratchCNNClassifier:\n",
    "\n",
    "    def __init__(self, num_epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, verbose=True, Activater=Tanh, Optimizer=AdaGrad):\n",
    "        self.num_epoch = num_epoch\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.Activater = Activater\n",
    "        if Activater == Sigmoid or Activater == Tanh:\n",
    "            self.Initializer = XavierInitializer\n",
    "        elif Activater == ReLU:\n",
    "            self.Initializer = HeInitializer\n",
    "        self.Optimizer = Optimizer\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.val_enable = False\n",
    "        if X_val is not None:\n",
    "            self.val_enable = True\n",
    "        self.Conv1d_Arbitrary_Strides = Conv1d_Arbitrary_Strides(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=3, stride=2)\n",
    "        self.Conv1d_Arbitrary_Strides.n_out = output_size_calculation(X.shape[-1], self.Conv1d_Arbitrary_Strides.b_size, self.Conv1d_Arbitrary_Strides.pa, self.Conv1d_Arbitrary_Strides.stride)\n",
    "        self.activation1 = self.Activater()\n",
    "        self.FC2 = FC(1*self.Conv1d_Arbitrary_Strides.n_out, self.n_nodes2, self.Initializer(), self.Optimizer(self.lr))\n",
    "        self.activation2 = self.Activater()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(), self.Optimizer(self.lr))\n",
    "        self.activation3 = Softmax()\n",
    "\n",
    "        self.loss = []\n",
    "        self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\n",
    "\n",
    "        for _ in range(self.num_epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            self.iter = len(get_mini_batch)\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                self.forward_propagation(mini_X)\n",
    "                self.back_propagation(mini_X, mini_y)\n",
    "                self.loss.append(self.activation3.loss)\n",
    "            self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward_propagation(X), axis=1)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        A1 = self.Conv1d_Arbitrary_Strides.forward(X)\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return Z3\n",
    "\n",
    "    def back_propagation(self, X, y_true):\n",
    "        dA3 = self.activation3.backward(y_true)\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dA1 = dA1[:, np.newaxis]\n",
    "        dZ0 = self.Conv1d_Arbitrary_Strides.backward(dA1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mO70kGJApgJ7",
   "metadata": {
    "id": "mO70kGJApgJ7"
   },
   "outputs": [],
   "source": [
    "scratch_cnn1d = ScratchCNNClassifier(num_epoch=20, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, verbose=True, Activater=Tanh, Optimizer=SGD)\n",
    "scratch_cnn1d.fit(X_train, y_train_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sLjUZobwpgqY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLjUZobwpgqY",
    "outputId": "264f467c-8c76-4d01-ae0d-c330b933e7de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9793"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cnn1.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
