{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andervies/divic-corp-machine-learning-course/blob/main/assignment28/Keras_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem One: Sharing and executing the official tutorial model"
      ],
      "metadata": {
        "id": "D9I8Ea3cK-au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import TensorFlow to get started"
      ],
      "metadata": {
        "id": "lQoxJVFhUPrz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0trJmd6DjqBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0514bd9d-2dfc-490d-d9d9-c84d4ed38994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare the MNIST dataset. The pixel values of the images range from 0 through 255. Scale these values to a range of 0 to 1 by dividing the values by 255.0. This also converts the sample data from integers to floating-point numbers:"
      ],
      "metadata": {
        "id": "y_FPZM-BT10D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7FP5258xjs-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8576a656-fe6a-4664-aaf6-bf8a902c19a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a tf.keras.Sequential model"
      ],
      "metadata": {
        "id": "HqVxzHO6UjG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making predictions using the untrained model"
      ],
      "metadata": {
        "id": "vKQKZ7UpUvde"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OeOrNdnkEEcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb1a9e8-717b-49b4-9cb6-36f81aed5b90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.09701695, -0.42782122,  0.12474165,  0.4498067 , -0.7428633 ,\n",
              "         0.23040389,  0.48080975, -0.23871507,  0.49615145, -0.4797041 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tf.nn.softmax function converts these logits to probabilities for each class"
      ],
      "metadata": {
        "id": "z8O7fJUIU4AR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zWSRnQ0WI5eq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc800b79-f96b-4590-b479-1a227fea2dcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.10150041, 0.06005283, 0.10435385, 0.14443833, 0.04382404,\n",
              "        0.11598371, 0.14898649, 0.07255398, 0.15128982, 0.05701656]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQyugpgRIyrA"
      },
      "source": [
        "Define a loss function for training using `losses.SparseCategoricalCrossentropy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RSkzdv8MD0tT"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to -tf.math.log(1/10) ~= 2.3"
      ],
      "metadata": {
        "id": "Y0TJ5jjVVIGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NJWqEVrrJ7ZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ada251e-5dcc-4788-d16f-b3ff0c8dc591"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1543055"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring and compiling the model using Keras Model.compile before beginning training"
      ],
      "metadata": {
        "id": "YECU5t9DVQRc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9foNKHzTD2Vo"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Model.fit method to adjust model parameters and minimize the loss"
      ],
      "metadata": {
        "id": "FtANqUkNVim7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y7suUbJXVLqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2eddc64-3a79-4d90-bc23-1f955b3c9b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 0.2935 - accuracy: 0.9147\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1436 - accuracy: 0.9571\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1075 - accuracy: 0.9674\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0866 - accuracy: 0.9732\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0762 - accuracy: 0.9760\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cea0227a140>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Model.evaluate method checks the model's performance"
      ],
      "metadata": {
        "id": "RDOEMu9aVtS1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "F7dTAzgHDUh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bcaaeb-f276-424a-8cac-07c269b957c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0729 - accuracy: 0.9792 - 687ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07294416427612305, 0.979200005531311]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get our model to return a probability, we can wrap the trained model in the Sequential class, and attach the softmax to it"
      ],
      "metadata": {
        "id": "tUojHlKgV1fQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rYb6DrEH0GMv"
      },
      "outputs": [],
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cnqOZtUp1YR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b017c9f1-c986-415e-a947-4adca732f3c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[1.6523332e-08, 1.4008761e-08, 4.3974401e-06, 5.8098417e-04,\n",
              "        6.9921437e-11, 2.4120897e-07, 4.5561482e-13, 9.9941361e-01,\n",
              "        2.1695135e-07, 5.5293322e-07],\n",
              "       [4.1952379e-07, 2.0351608e-05, 9.9994695e-01, 2.9258730e-05,\n",
              "        4.6562450e-13, 9.5160954e-07, 3.2780616e-08, 1.8639070e-12,\n",
              "        2.0479163e-06, 1.0462100e-11],\n",
              "       [6.9884862e-07, 9.9767119e-01, 4.2209239e-04, 5.8585880e-05,\n",
              "        3.8834587e-05, 4.1444027e-06, 9.8606233e-06, 1.2903715e-03,\n",
              "        5.0147570e-04, 2.7855110e-06],\n",
              "       [9.9992609e-01, 4.8043449e-09, 5.8983205e-05, 6.5251626e-08,\n",
              "        5.1328268e-08, 1.6541933e-06, 1.1312318e-05, 1.4476245e-06,\n",
              "        1.6638952e-08, 3.6356832e-07],\n",
              "       [6.4561045e-06, 2.0696733e-08, 1.9558477e-06, 1.1374278e-08,\n",
              "        9.9749005e-01, 7.6075928e-08, 1.8874531e-05, 1.1320080e-03,\n",
              "        4.5496768e-06, 1.3459733e-03]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "probability_model(x_test[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trains to ~98% accuracy on this dataset and by applying the softmax function on the outputs of the trained model, we can make probabilistic classifications."
      ],
      "metadata": {
        "id": "-W0Ovv7ZBFmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Three: Learning Iris (binary classification) with Keras"
      ],
      "metadata": {
        "id": "-PSzpSlZBGQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "sjdK9MJEdoH2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "iris_df = pd.read_csv(\"/content/drive/MyDrive/assignment28/Iris.csv\")\n",
        "\n",
        "# Filter only versicolor and virginica species\n",
        "iris_df = iris_df[(iris_df[\"Species\"] == \"Iris-versicolor\") | (iris_df[\"Species\"] == \"Iris-virginica\")]\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris_df[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
        "y = iris_df[\"Species\"].replace({\"Iris-versicolor\": 0, \"Iris-virginica\": 1}).astype(np.int64).values\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "X_test, y_test = X_temp, y_temp\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "# Neural network model\n",
        "model = Sequential([\n",
        "    Dense(50, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_QESNFSBIgv",
        "outputId": "2a18c219-6740-4513-97fb-47846206a02a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 1s 40ms/step - loss: 0.6820 - accuracy: 0.5312 - val_loss: 0.6831 - val_accuracy: 0.4375\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6496 - accuracy: 0.5938 - val_loss: 0.6666 - val_accuracy: 0.4375\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6288 - accuracy: 0.5469 - val_loss: 0.6634 - val_accuracy: 0.4375\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6098 - accuracy: 0.6406 - val_loss: 0.6229 - val_accuracy: 0.7500\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5858 - accuracy: 0.7812 - val_loss: 0.6131 - val_accuracy: 0.6875\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.5723 - accuracy: 0.8125 - val_loss: 0.5898 - val_accuracy: 0.7500\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5505 - accuracy: 0.8281 - val_loss: 0.5767 - val_accuracy: 0.7500\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5313 - accuracy: 0.7969 - val_loss: 0.5872 - val_accuracy: 0.5000\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5256 - accuracy: 0.7500 - val_loss: 0.5486 - val_accuracy: 0.7500\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5061 - accuracy: 0.9062 - val_loss: 0.4975 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4838 - accuracy: 0.9375 - val_loss: 0.5202 - val_accuracy: 0.8125\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4720 - accuracy: 0.8281 - val_loss: 0.5069 - val_accuracy: 0.8125\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4459 - accuracy: 0.9375 - val_loss: 0.4462 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4335 - accuracy: 0.9844 - val_loss: 0.4359 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4317 - accuracy: 0.8438 - val_loss: 0.4828 - val_accuracy: 0.8125\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4049 - accuracy: 0.8750 - val_loss: 0.4097 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3922 - accuracy: 0.9844 - val_loss: 0.3823 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3658 - accuracy: 0.9844 - val_loss: 0.3859 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3622 - accuracy: 0.9219 - val_loss: 0.3949 - val_accuracy: 0.8750\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3424 - accuracy: 0.9219 - val_loss: 0.3342 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3210 - accuracy: 0.9688 - val_loss: 0.3302 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3070 - accuracy: 0.9688 - val_loss: 0.3192 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2939 - accuracy: 0.9688 - val_loss: 0.2901 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2812 - accuracy: 0.9844 - val_loss: 0.2816 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.2755 - accuracy: 0.9375 - val_loss: 0.2868 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.2585 - accuracy: 0.9688 - val_loss: 0.2503 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2487 - accuracy: 0.9688 - val_loss: 0.2517 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2345 - accuracy: 0.9688 - val_loss: 0.2324 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2262 - accuracy: 0.9688 - val_loss: 0.2262 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2221 - accuracy: 0.9531 - val_loss: 0.2183 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2162 - accuracy: 0.9844 - val_loss: 0.1984 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2004 - accuracy: 0.9844 - val_loss: 0.2032 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1941 - accuracy: 0.9688 - val_loss: 0.1951 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.1891 - accuracy: 0.9688 - val_loss: 0.1837 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.1806 - accuracy: 0.9688 - val_loss: 0.1707 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1939 - accuracy: 0.9219 - val_loss: 0.1624 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1660 - accuracy: 0.9688 - val_loss: 0.1832 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1680 - accuracy: 0.9375 - val_loss: 0.1514 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1580 - accuracy: 0.9844 - val_loss: 0.1462 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1540 - accuracy: 0.9688 - val_loss: 0.1415 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1505 - accuracy: 0.9688 - val_loss: 0.1423 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.1448 - accuracy: 0.9688 - val_loss: 0.1327 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.1427 - accuracy: 0.9844 - val_loss: 0.1285 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.1391 - accuracy: 0.9688 - val_loss: 0.1277 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1538 - accuracy: 0.9375 - val_loss: 0.1342 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1339 - accuracy: 0.9688 - val_loss: 0.1179 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1355 - accuracy: 0.9844 - val_loss: 0.1144 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1341 - accuracy: 0.9688 - val_loss: 0.1168 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1405 - accuracy: 0.9375 - val_loss: 0.1150 - val_accuracy: 0.9375\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1237 - accuracy: 0.9844 - val_loss: 0.1158 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1248 - accuracy: 0.9688 - val_loss: 0.1037 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1172 - accuracy: 0.9688 - val_loss: 0.1021 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1182 - accuracy: 0.9688 - val_loss: 0.1024 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.1168 - accuracy: 0.9688 - val_loss: 0.0965 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.1095 - accuracy: 0.9688 - val_loss: 0.1000 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.1145 - accuracy: 0.9688 - val_loss: 0.0929 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1154 - accuracy: 0.9688 - val_loss: 0.0931 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1087 - accuracy: 0.9688 - val_loss: 0.0895 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1087 - accuracy: 0.9688 - val_loss: 0.0872 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1019 - accuracy: 0.9688 - val_loss: 0.0880 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1046 - accuracy: 0.9688 - val_loss: 0.0841 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0974 - accuracy: 0.9688 - val_loss: 0.0837 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1024 - accuracy: 0.9844 - val_loss: 0.0804 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0989 - accuracy: 0.9688 - val_loss: 0.0882 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.1042 - accuracy: 0.9688 - val_loss: 0.0792 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1068 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0987 - accuracy: 0.9688 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0935 - accuracy: 0.9688 - val_loss: 0.0747 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.1006 - accuracy: 0.9844 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0879 - accuracy: 0.9844 - val_loss: 0.0755 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0992 - accuracy: 0.9688 - val_loss: 0.0702 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.1104 - accuracy: 0.9531 - val_loss: 0.0783 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0948 - accuracy: 0.9844 - val_loss: 0.0900 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.1022 - accuracy: 0.9688 - val_loss: 0.0664 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.0913 - accuracy: 0.9844 - val_loss: 0.0822 - val_accuracy: 0.9375\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0994 - accuracy: 0.9688 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0849 - accuracy: 0.9688 - val_loss: 0.0643 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0835 - accuracy: 0.9688 - val_loss: 0.0623 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0847 - accuracy: 0.9688 - val_loss: 0.0614 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0922 - accuracy: 0.9688 - val_loss: 0.0634 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0908 - accuracy: 0.9688 - val_loss: 0.0729 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0900 - accuracy: 0.9688 - val_loss: 0.0669 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1023 - accuracy: 0.9688 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0879 - accuracy: 0.9688 - val_loss: 0.0589 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0770 - accuracy: 0.9844 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0853 - accuracy: 0.9844 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0787 - accuracy: 0.9688 - val_loss: 0.0551 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0786 - accuracy: 0.9688 - val_loss: 0.0576 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.1279 - accuracy: 0.9062 - val_loss: 0.0697 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0981 - accuracy: 0.9375 - val_loss: 0.0782 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0821 - accuracy: 0.9688 - val_loss: 0.0575 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.0895 - accuracy: 0.9688 - val_loss: 0.0757 - val_accuracy: 0.9375\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.1013 - accuracy: 0.9688 - val_loss: 0.0558 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0784 - accuracy: 0.9688 - val_loss: 0.0520 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0782 - accuracy: 1.0000 - val_loss: 0.0537 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.0721 - accuracy: 0.9844 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0796 - accuracy: 0.9688 - val_loss: 0.0491 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.0690 - accuracy: 0.9688 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.0923 - accuracy: 0.9844 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2220 - accuracy: 0.9000\n",
            "Test Accuracy: 0.8999999761581421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Four: Learn Iris (multi-level classification) with Keras"
      ],
      "metadata": {
        "id": "F2qp0KZMcm7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "iris_df = pd.read_csv(\"/content/drive/MyDrive/assignment28/Iris.csv\")\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris_df[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
        "y = iris_df[\"Species\"].replace({\"Iris-versicolor\": 0, \"Iris-virginica\": 1, \"Iris-setosa\": 2}).values\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "X_test, y_test = X_temp, y_temp\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "# Neural network model\n",
        "model = Sequential([\n",
        "    Dense(50, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHsio3LIFE7T",
        "outputId": "0d419b24-cdf6-437c-8217-aa18a41ad398"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 24ms/step - loss: 1.0667 - accuracy: 0.3646 - val_loss: 0.9577 - val_accuracy: 0.3750\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.9015 - accuracy: 0.5833 - val_loss: 0.8441 - val_accuracy: 0.7083\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.7813 - accuracy: 0.6875 - val_loss: 0.7304 - val_accuracy: 0.7083\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.6731 - accuracy: 0.8333 - val_loss: 0.6403 - val_accuracy: 0.7083\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.5677 - accuracy: 0.7083 - val_loss: 0.5532 - val_accuracy: 0.7083\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.4861 - accuracy: 0.8125 - val_loss: 0.4899 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.4388 - accuracy: 0.9583 - val_loss: 0.4424 - val_accuracy: 0.8333\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.3809 - accuracy: 0.8333 - val_loss: 0.4252 - val_accuracy: 0.7083\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.3525 - accuracy: 0.9479 - val_loss: 0.3813 - val_accuracy: 0.9167\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.9062 - val_loss: 0.3655 - val_accuracy: 0.9167\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2782 - accuracy: 0.9792 - val_loss: 0.3336 - val_accuracy: 0.9167\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2636 - accuracy: 0.9271 - val_loss: 0.3217 - val_accuracy: 0.9167\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.2275 - accuracy: 0.9688 - val_loss: 0.2957 - val_accuracy: 0.9167\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.2142 - accuracy: 0.9583 - val_loss: 0.2845 - val_accuracy: 0.9167\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1909 - accuracy: 0.9792 - val_loss: 0.2677 - val_accuracy: 0.9167\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1772 - accuracy: 0.9792 - val_loss: 0.2707 - val_accuracy: 0.9167\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1787 - accuracy: 0.9688 - val_loss: 0.2500 - val_accuracy: 0.9167\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1712 - accuracy: 0.9583 - val_loss: 0.2392 - val_accuracy: 0.9167\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1470 - accuracy: 0.9792 - val_loss: 0.2332 - val_accuracy: 0.9167\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1402 - accuracy: 0.9688 - val_loss: 0.2226 - val_accuracy: 0.9167\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.1436 - accuracy: 0.9688 - val_loss: 0.2243 - val_accuracy: 0.9167\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1426 - accuracy: 0.9583 - val_loss: 0.2181 - val_accuracy: 0.9167\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1159 - accuracy: 0.9688 - val_loss: 0.2199 - val_accuracy: 0.9167\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1108 - accuracy: 0.9688 - val_loss: 0.2077 - val_accuracy: 0.9167\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1040 - accuracy: 0.9792 - val_loss: 0.2036 - val_accuracy: 0.9167\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1021 - accuracy: 0.9792 - val_loss: 0.1994 - val_accuracy: 0.9167\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0981 - accuracy: 0.9792 - val_loss: 0.2009 - val_accuracy: 0.9167\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.1054 - accuracy: 0.9583 - val_loss: 0.2153 - val_accuracy: 0.9167\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0917 - accuracy: 0.9688 - val_loss: 0.1946 - val_accuracy: 0.9583\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0963 - accuracy: 0.9688 - val_loss: 0.2205 - val_accuracy: 0.9167\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0966 - accuracy: 0.9688 - val_loss: 0.1852 - val_accuracy: 0.9167\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0917 - accuracy: 0.9792 - val_loss: 0.1961 - val_accuracy: 0.9167\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0857 - accuracy: 0.9688 - val_loss: 0.1852 - val_accuracy: 0.9167\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0806 - accuracy: 0.9688 - val_loss: 0.2122 - val_accuracy: 0.9167\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0846 - accuracy: 0.9583 - val_loss: 0.1835 - val_accuracy: 0.9167\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9688 - val_loss: 0.1988 - val_accuracy: 0.9167\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.9792 - val_loss: 0.1858 - val_accuracy: 0.9167\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0780 - accuracy: 0.9792 - val_loss: 0.2050 - val_accuracy: 0.9167\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0822 - accuracy: 0.9688 - val_loss: 0.1887 - val_accuracy: 0.9167\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0816 - accuracy: 0.9583 - val_loss: 0.1808 - val_accuracy: 0.9167\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0713 - accuracy: 0.9792 - val_loss: 0.1805 - val_accuracy: 0.9167\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0631 - accuracy: 0.9792 - val_loss: 0.1995 - val_accuracy: 0.9167\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0690 - accuracy: 0.9688 - val_loss: 0.1795 - val_accuracy: 0.9167\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0674 - accuracy: 0.9688 - val_loss: 0.1870 - val_accuracy: 0.9167\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0785 - accuracy: 0.9688 - val_loss: 0.1991 - val_accuracy: 0.9167\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0973 - accuracy: 0.9583 - val_loss: 0.2010 - val_accuracy: 0.9167\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0655 - accuracy: 0.9896 - val_loss: 0.1721 - val_accuracy: 0.9583\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0597 - accuracy: 0.9896 - val_loss: 0.2099 - val_accuracy: 0.9167\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0802 - accuracy: 0.9583 - val_loss: 0.1727 - val_accuracy: 0.9167\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0791 - accuracy: 0.9583 - val_loss: 0.1997 - val_accuracy: 0.9167\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0647 - accuracy: 0.9688 - val_loss: 0.1925 - val_accuracy: 0.9167\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0551 - accuracy: 0.9896 - val_loss: 0.1812 - val_accuracy: 0.9167\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0556 - accuracy: 0.9896 - val_loss: 0.1923 - val_accuracy: 0.9167\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0618 - accuracy: 0.9688 - val_loss: 0.1801 - val_accuracy: 0.9167\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0538 - accuracy: 0.9896 - val_loss: 0.2033 - val_accuracy: 0.9167\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0584 - accuracy: 0.9792 - val_loss: 0.1784 - val_accuracy: 0.9167\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0522 - accuracy: 0.9896 - val_loss: 0.2002 - val_accuracy: 0.9167\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0671 - accuracy: 0.9688 - val_loss: 0.1792 - val_accuracy: 0.9167\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0639 - accuracy: 0.9688 - val_loss: 0.2144 - val_accuracy: 0.9167\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9896 - val_loss: 0.1705 - val_accuracy: 0.9583\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0580 - accuracy: 0.9688 - val_loss: 0.2171 - val_accuracy: 0.9167\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0531 - accuracy: 0.9792 - val_loss: 0.1903 - val_accuracy: 0.9167\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0527 - accuracy: 0.9896 - val_loss: 0.1855 - val_accuracy: 0.9167\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0538 - accuracy: 0.9792 - val_loss: 0.1860 - val_accuracy: 0.9167\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0541 - accuracy: 0.9792 - val_loss: 0.2252 - val_accuracy: 0.9167\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0496 - accuracy: 0.9896 - val_loss: 0.1867 - val_accuracy: 0.9167\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0504 - accuracy: 0.9896 - val_loss: 0.1896 - val_accuracy: 0.9167\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0462 - accuracy: 0.9896 - val_loss: 0.2202 - val_accuracy: 0.9167\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0585 - accuracy: 0.9583 - val_loss: 0.1761 - val_accuracy: 0.9167\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0551 - accuracy: 0.9688 - val_loss: 0.2137 - val_accuracy: 0.9167\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.9896 - val_loss: 0.1850 - val_accuracy: 0.9167\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0514 - accuracy: 0.9792 - val_loss: 0.2185 - val_accuracy: 0.9167\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0482 - accuracy: 0.9896 - val_loss: 0.1816 - val_accuracy: 0.9167\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0459 - accuracy: 0.9896 - val_loss: 0.2030 - val_accuracy: 0.9167\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0444 - accuracy: 0.9896 - val_loss: 0.1919 - val_accuracy: 0.9167\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0431 - accuracy: 0.9792 - val_loss: 0.2102 - val_accuracy: 0.9167\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0445 - accuracy: 0.9896 - val_loss: 0.1852 - val_accuracy: 0.9167\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.9792 - val_loss: 0.2038 - val_accuracy: 0.9167\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0449 - accuracy: 0.9896 - val_loss: 0.1985 - val_accuracy: 0.9167\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.9792 - val_loss: 0.1935 - val_accuracy: 0.9167\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0468 - accuracy: 0.9896 - val_loss: 0.2403 - val_accuracy: 0.9167\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0530 - accuracy: 0.9688 - val_loss: 0.1845 - val_accuracy: 0.9167\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0455 - accuracy: 0.9896 - val_loss: 0.1917 - val_accuracy: 0.9167\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9896 - val_loss: 0.2532 - val_accuracy: 0.9167\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0421 - accuracy: 0.9792 - val_loss: 0.1780 - val_accuracy: 0.9167\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0516 - accuracy: 0.9792 - val_loss: 0.2408 - val_accuracy: 0.9167\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0485 - accuracy: 0.9792 - val_loss: 0.1816 - val_accuracy: 0.9167\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 0.9167\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0515 - accuracy: 0.9792 - val_loss: 0.1975 - val_accuracy: 0.9167\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0405 - accuracy: 0.9896 - val_loss: 0.2280 - val_accuracy: 0.9167\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0381 - accuracy: 0.9896 - val_loss: 0.1966 - val_accuracy: 0.9167\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0408 - accuracy: 0.9792 - val_loss: 0.2090 - val_accuracy: 0.9167\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0405 - accuracy: 0.9896 - val_loss: 0.2131 - val_accuracy: 0.9167\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0459 - accuracy: 0.9688 - val_loss: 0.2096 - val_accuracy: 0.9167\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0434 - accuracy: 0.9688 - val_loss: 0.2033 - val_accuracy: 0.9167\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0386 - accuracy: 0.9896 - val_loss: 0.2148 - val_accuracy: 0.9167\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0428 - accuracy: 0.9896 - val_loss: 0.2306 - val_accuracy: 0.9167\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0411 - accuracy: 0.9792 - val_loss: 0.2011 - val_accuracy: 0.9167\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 0.9896 - val_loss: 0.2358 - val_accuracy: 0.9167\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.9896 - val_loss: 0.1769 - val_accuracy: 0.9167\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0286 - accuracy: 0.9667\n",
            "Test Accuracy: 0.9666666388511658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Five: Learning House Prices with Keras"
      ],
      "metadata": {
        "id": "RJjByh_bcwh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "house_df = pd.read_csv(\"/content/drive/MyDrive/assignment28/train.csv\")\n",
        "\n",
        "# Extract features and labels\n",
        "X = house_df[[\"GrLivArea\", \"YearBuilt\"]].values\n",
        "y = house_df[\"SalePrice\"].values\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "X_test, y_test = X_temp, y_temp\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "# Neural network model\n",
        "model = Sequential([\n",
        "    Dense(50, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print(\"Test MSE:\", test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxneAwk6FdiU",
        "outputId": "11db6566-4fb0-4e3f-f1e5-1a41c53e2068"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 37208379392.0000 - val_loss: 30877843456.0000\n",
            "Epoch 2/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 21745981440.0000 - val_loss: 7343503872.0000\n",
            "Epoch 3/100\n",
            "94/94 [==============================] - 1s 8ms/step - loss: 4725260800.0000 - val_loss: 3205627392.0000\n",
            "Epoch 4/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3865221888.0000 - val_loss: 3157396224.0000\n",
            "Epoch 5/100\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 3787299840.0000 - val_loss: 3187352320.0000\n",
            "Epoch 6/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3720390144.0000 - val_loss: 2950095360.0000\n",
            "Epoch 7/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3634663424.0000 - val_loss: 2874524160.0000\n",
            "Epoch 8/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3568010240.0000 - val_loss: 2866650112.0000\n",
            "Epoch 9/100\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 3515464448.0000 - val_loss: 2767254528.0000\n",
            "Epoch 10/100\n",
            "94/94 [==============================] - 1s 5ms/step - loss: 3439336960.0000 - val_loss: 2706352896.0000\n",
            "Epoch 11/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3373837312.0000 - val_loss: 2722601472.0000\n",
            "Epoch 12/100\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 3339723520.0000 - val_loss: 2639835136.0000\n",
            "Epoch 13/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3297099776.0000 - val_loss: 2598717184.0000\n",
            "Epoch 14/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3260986880.0000 - val_loss: 2597620992.0000\n",
            "Epoch 15/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3231778816.0000 - val_loss: 2539343616.0000\n",
            "Epoch 16/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3203761920.0000 - val_loss: 2525885184.0000\n",
            "Epoch 17/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3187026176.0000 - val_loss: 2483686656.0000\n",
            "Epoch 18/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3154727936.0000 - val_loss: 2503369984.0000\n",
            "Epoch 19/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3152333568.0000 - val_loss: 2466096896.0000\n",
            "Epoch 20/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3135021568.0000 - val_loss: 2485909504.0000\n",
            "Epoch 21/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3126156800.0000 - val_loss: 2450692864.0000\n",
            "Epoch 22/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3098167552.0000 - val_loss: 2514205952.0000\n",
            "Epoch 23/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3086733824.0000 - val_loss: 2433712128.0000\n",
            "Epoch 24/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3088233472.0000 - val_loss: 2456117760.0000\n",
            "Epoch 25/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3088635392.0000 - val_loss: 2430128384.0000\n",
            "Epoch 26/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3063843840.0000 - val_loss: 2586217216.0000\n",
            "Epoch 27/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3087655680.0000 - val_loss: 2503902464.0000\n",
            "Epoch 28/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3103565312.0000 - val_loss: 2432688128.0000\n",
            "Epoch 29/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 3072465664.0000 - val_loss: 2441110528.0000\n",
            "Epoch 30/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3079424256.0000 - val_loss: 2435592192.0000\n",
            "Epoch 31/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3088911360.0000 - val_loss: 2455073280.0000\n",
            "Epoch 32/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3071198464.0000 - val_loss: 2428884992.0000\n",
            "Epoch 33/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3109420544.0000 - val_loss: 2431163136.0000\n",
            "Epoch 34/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 3080665600.0000 - val_loss: 2472917248.0000\n",
            "Epoch 35/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3072655616.0000 - val_loss: 2461877760.0000\n",
            "Epoch 36/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3068957440.0000 - val_loss: 2438478592.0000\n",
            "Epoch 37/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 3076032768.0000 - val_loss: 2481391872.0000\n",
            "Epoch 38/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3081377024.0000 - val_loss: 2501589760.0000\n",
            "Epoch 39/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3066310912.0000 - val_loss: 2439058432.0000\n",
            "Epoch 40/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3076573696.0000 - val_loss: 2447110144.0000\n",
            "Epoch 41/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3094563328.0000 - val_loss: 2472765952.0000\n",
            "Epoch 42/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3073186048.0000 - val_loss: 2444707072.0000\n",
            "Epoch 43/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3063741440.0000 - val_loss: 2437226240.0000\n",
            "Epoch 44/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3072322304.0000 - val_loss: 2434732544.0000\n",
            "Epoch 45/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3071221760.0000 - val_loss: 2434023680.0000\n",
            "Epoch 46/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3085361664.0000 - val_loss: 2470951424.0000\n",
            "Epoch 47/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3069764608.0000 - val_loss: 2496965888.0000\n",
            "Epoch 48/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3068157952.0000 - val_loss: 2441211392.0000\n",
            "Epoch 49/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3097048064.0000 - val_loss: 2474777344.0000\n",
            "Epoch 50/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3065536000.0000 - val_loss: 2474868736.0000\n",
            "Epoch 51/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3073662464.0000 - val_loss: 2437733632.0000\n",
            "Epoch 52/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3078634240.0000 - val_loss: 2469816320.0000\n",
            "Epoch 53/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3069802496.0000 - val_loss: 2452705536.0000\n",
            "Epoch 54/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3059159552.0000 - val_loss: 2571172096.0000\n",
            "Epoch 55/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3074743296.0000 - val_loss: 2473466368.0000\n",
            "Epoch 56/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3080317952.0000 - val_loss: 2444326144.0000\n",
            "Epoch 57/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3059319808.0000 - val_loss: 2580215296.0000\n",
            "Epoch 58/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3080118272.0000 - val_loss: 2462130688.0000\n",
            "Epoch 59/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3093447424.0000 - val_loss: 2495975168.0000\n",
            "Epoch 60/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3077370368.0000 - val_loss: 2441083904.0000\n",
            "Epoch 61/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3069442048.0000 - val_loss: 2505198080.0000\n",
            "Epoch 62/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3070412288.0000 - val_loss: 2460620032.0000\n",
            "Epoch 63/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3066507520.0000 - val_loss: 2585952256.0000\n",
            "Epoch 64/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3076737536.0000 - val_loss: 2457343232.0000\n",
            "Epoch 65/100\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 3083782400.0000 - val_loss: 2475789568.0000\n",
            "Epoch 66/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3077074688.0000 - val_loss: 2448791040.0000\n",
            "Epoch 67/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3089059072.0000 - val_loss: 2451133952.0000\n",
            "Epoch 68/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3098642176.0000 - val_loss: 2486631936.0000\n",
            "Epoch 69/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3079868928.0000 - val_loss: 2525227776.0000\n",
            "Epoch 70/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3073615104.0000 - val_loss: 2452382720.0000\n",
            "Epoch 71/100\n",
            "94/94 [==============================] - 0s 4ms/step - loss: 3070046464.0000 - val_loss: 2435686400.0000\n",
            "Epoch 72/100\n",
            "94/94 [==============================] - 0s 5ms/step - loss: 3084008448.0000 - val_loss: 2442839040.0000\n",
            "Epoch 73/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3080620032.0000 - val_loss: 2454684160.0000\n",
            "Epoch 74/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3071325184.0000 - val_loss: 2435230208.0000\n",
            "Epoch 75/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3090994176.0000 - val_loss: 2438237696.0000\n",
            "Epoch 76/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3076556800.0000 - val_loss: 2442644736.0000\n",
            "Epoch 77/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3060919808.0000 - val_loss: 2543750400.0000\n",
            "Epoch 78/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3093863424.0000 - val_loss: 2458002432.0000\n",
            "Epoch 79/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3090617344.0000 - val_loss: 2449938944.0000\n",
            "Epoch 80/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3077416448.0000 - val_loss: 2521324544.0000\n",
            "Epoch 81/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3074821120.0000 - val_loss: 2448356096.0000\n",
            "Epoch 82/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3073404160.0000 - val_loss: 2469634816.0000\n",
            "Epoch 83/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3067090176.0000 - val_loss: 2447513344.0000\n",
            "Epoch 84/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3066298112.0000 - val_loss: 2438059008.0000\n",
            "Epoch 85/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3063872512.0000 - val_loss: 2485745408.0000\n",
            "Epoch 86/100\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 3062169088.0000 - val_loss: 2545624320.0000\n",
            "Epoch 87/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3087693824.0000 - val_loss: 2434537984.0000\n",
            "Epoch 88/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3077050880.0000 - val_loss: 2435296512.0000\n",
            "Epoch 89/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3085146368.0000 - val_loss: 2471162880.0000\n",
            "Epoch 90/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3073454848.0000 - val_loss: 2461753344.0000\n",
            "Epoch 91/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3078055168.0000 - val_loss: 2486287616.0000\n",
            "Epoch 92/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3085670912.0000 - val_loss: 2481040896.0000\n",
            "Epoch 93/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3072345600.0000 - val_loss: 2444289792.0000\n",
            "Epoch 94/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3073874176.0000 - val_loss: 2440159744.0000\n",
            "Epoch 95/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3075364864.0000 - val_loss: 2469366784.0000\n",
            "Epoch 96/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3071972096.0000 - val_loss: 2508292352.0000\n",
            "Epoch 97/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3071787008.0000 - val_loss: 2438434304.0000\n",
            "Epoch 98/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3062816512.0000 - val_loss: 2443715072.0000\n",
            "Epoch 99/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3079938048.0000 - val_loss: 2461750528.0000\n",
            "Epoch 100/100\n",
            "94/94 [==============================] - 0s 3ms/step - loss: 3087663872.0000 - val_loss: 2448828160.0000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 3873921280.0000\n",
            "Test MSE: 3873921280.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Six: Learning MNIST with Keras"
      ],
      "metadata": {
        "id": "PCKYPslJc_o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize data\n",
        "X_train = X_train.reshape(-1, 784).astype('float32') / 255\n",
        "X_test = X_test.reshape(-1, 784).astype('float32') / 255\n",
        "\n",
        "# One-hot encode labels\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_one_hot = enc.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "# Neural network model\n",
        "model = Sequential([\n",
        "    Dense(50, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_one_hot, epochs=num_epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test_one_hot)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_dxZlBZGtID",
        "outputId": "1fc31baf-d59e-4063-a45a-ec86a0c38331"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4800/4800 [==============================] - 17s 3ms/step - loss: 0.2686 - accuracy: 0.9187 - val_loss: 0.1505 - val_accuracy: 0.9578\n",
            "Epoch 2/10\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.1272 - accuracy: 0.9618 - val_loss: 0.1244 - val_accuracy: 0.9629\n",
            "Epoch 3/10\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0946 - accuracy: 0.9703 - val_loss: 0.1027 - val_accuracy: 0.9692\n",
            "Epoch 4/10\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0746 - accuracy: 0.9761 - val_loss: 0.1048 - val_accuracy: 0.9700\n",
            "Epoch 5/10\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.0608 - accuracy: 0.9800 - val_loss: 0.1147 - val_accuracy: 0.9678\n",
            "Epoch 6/10\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.0528 - accuracy: 0.9827 - val_loss: 0.1063 - val_accuracy: 0.9728\n",
            "Epoch 7/10\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.0467 - accuracy: 0.9849 - val_loss: 0.1081 - val_accuracy: 0.9737\n",
            "Epoch 8/10\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0412 - accuracy: 0.9864 - val_loss: 0.1302 - val_accuracy: 0.9697\n",
            "Epoch 9/10\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0334 - accuracy: 0.9889 - val_loss: 0.1251 - val_accuracy: 0.9718\n",
            "Epoch 10/10\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.0338 - accuracy: 0.9889 - val_loss: 0.1305 - val_accuracy: 0.9698\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1324 - accuracy: 0.9707\n",
            "Test Loss: 0.1323888599872589\n",
            "Test Accuracy: 0.9707000255584717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Seven: (Advance assignment) Rewriting to PyTorch"
      ],
      "metadata": {
        "id": "o-gbLTuDih1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load dataset\n",
        "iris_df = pd.read_csv(\"/content/drive/MyDrive/assignment28/Iris.csv\")\n",
        "\n",
        "# Filter only versicolor and virginica species\n",
        "iris_df = iris_df[(iris_df[\"Species\"] == \"Iris-versicolor\") | (iris_df[\"Species\"] == \"Iris-virginica\")]\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris_df[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
        "y = iris_df[\"Species\"].replace({\"Iris-versicolor\": 0, \"Iris-virginica\": 1}).astype(np.int64).values\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "X_test, y_test = X_temp, y_temp\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Neural network model\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(X_train.shape[1], 50)\n",
        "        self.fc2 = nn.Linear(50, 100)\n",
        "        self.fc3 = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "model = BinaryClassifier()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            outputs = model(X_batch)\n",
        "            val_loss += criterion(outputs, y_batch).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test).item()\n",
        "    test_acc = ((test_outputs >= 0.5) == y_test).float().mean().item()\n",
        "    print(\"Test Loss:\", test_loss)\n",
        "    print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxotoYN_itW9",
        "outputId": "b12768ce-e4be-4c5e-b902-45535c12bfa6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.7177, Val Loss: 0.7103\n",
            "Epoch [2/100], Loss: 0.8101, Val Loss: 0.7143\n",
            "Epoch [3/100], Loss: 0.6155, Val Loss: 0.6926\n",
            "Epoch [4/100], Loss: 0.7101, Val Loss: 0.6873\n",
            "Epoch [5/100], Loss: 0.6177, Val Loss: 0.6797\n",
            "Epoch [6/100], Loss: 0.5816, Val Loss: 0.6528\n",
            "Epoch [7/100], Loss: 0.5190, Val Loss: 0.6857\n",
            "Epoch [8/100], Loss: 0.5599, Val Loss: 0.7012\n",
            "Epoch [9/100], Loss: 0.5859, Val Loss: 0.6786\n",
            "Epoch [10/100], Loss: 0.5696, Val Loss: 0.6285\n",
            "Epoch [11/100], Loss: 0.5245, Val Loss: 0.6268\n",
            "Epoch [12/100], Loss: 0.6203, Val Loss: 0.6389\n",
            "Epoch [13/100], Loss: 0.5968, Val Loss: 0.6054\n",
            "Epoch [14/100], Loss: 0.5048, Val Loss: 0.5750\n",
            "Epoch [15/100], Loss: 0.4752, Val Loss: 0.5791\n",
            "Epoch [16/100], Loss: 0.5210, Val Loss: 0.5319\n",
            "Epoch [17/100], Loss: 0.6976, Val Loss: 0.5919\n",
            "Epoch [18/100], Loss: 0.4387, Val Loss: 0.5152\n",
            "Epoch [19/100], Loss: 0.4169, Val Loss: 0.4978\n",
            "Epoch [20/100], Loss: 0.4613, Val Loss: 0.4951\n",
            "Epoch [21/100], Loss: 0.3805, Val Loss: 0.4650\n",
            "Epoch [22/100], Loss: 0.4537, Val Loss: 0.4797\n",
            "Epoch [23/100], Loss: 0.3893, Val Loss: 0.4350\n",
            "Epoch [24/100], Loss: 0.3712, Val Loss: 0.4593\n",
            "Epoch [25/100], Loss: 0.4324, Val Loss: 0.3863\n",
            "Epoch [26/100], Loss: 0.3749, Val Loss: 0.4087\n",
            "Epoch [27/100], Loss: 0.3125, Val Loss: 0.3670\n",
            "Epoch [28/100], Loss: 0.4337, Val Loss: 0.3492\n",
            "Epoch [29/100], Loss: 0.4102, Val Loss: 0.3434\n",
            "Epoch [30/100], Loss: 0.3751, Val Loss: 0.3500\n",
            "Epoch [31/100], Loss: 0.2959, Val Loss: 0.3040\n",
            "Epoch [32/100], Loss: 0.1737, Val Loss: 0.2896\n",
            "Epoch [33/100], Loss: 0.3128, Val Loss: 0.3131\n",
            "Epoch [34/100], Loss: 0.1993, Val Loss: 0.2613\n",
            "Epoch [35/100], Loss: 0.2762, Val Loss: 0.2860\n",
            "Epoch [36/100], Loss: 0.1874, Val Loss: 0.2552\n",
            "Epoch [37/100], Loss: 0.1105, Val Loss: 0.2458\n",
            "Epoch [38/100], Loss: 0.0777, Val Loss: 0.2584\n",
            "Epoch [39/100], Loss: 0.4716, Val Loss: 0.2076\n",
            "Epoch [40/100], Loss: 0.1948, Val Loss: 0.2258\n",
            "Epoch [41/100], Loss: 0.2159, Val Loss: 0.2514\n",
            "Epoch [42/100], Loss: 0.1760, Val Loss: 0.1939\n",
            "Epoch [43/100], Loss: 0.2281, Val Loss: 0.1872\n",
            "Epoch [44/100], Loss: 0.3099, Val Loss: 0.2043\n",
            "Epoch [45/100], Loss: 0.2022, Val Loss: 0.1889\n",
            "Epoch [46/100], Loss: 0.0653, Val Loss: 0.1646\n",
            "Epoch [47/100], Loss: 0.1314, Val Loss: 0.1736\n",
            "Epoch [48/100], Loss: 0.1656, Val Loss: 0.1734\n",
            "Epoch [49/100], Loss: 0.3500, Val Loss: 0.1510\n",
            "Epoch [50/100], Loss: 0.2487, Val Loss: 0.1907\n",
            "Epoch [51/100], Loss: 0.1953, Val Loss: 0.1469\n",
            "Epoch [52/100], Loss: 0.1927, Val Loss: 0.1473\n",
            "Epoch [53/100], Loss: 0.3064, Val Loss: 0.1536\n",
            "Epoch [54/100], Loss: 0.3275, Val Loss: 0.1345\n",
            "Epoch [55/100], Loss: 0.2122, Val Loss: 0.1674\n",
            "Epoch [56/100], Loss: 0.0625, Val Loss: 0.1223\n",
            "Epoch [57/100], Loss: 0.2033, Val Loss: 0.1269\n",
            "Epoch [58/100], Loss: 0.0395, Val Loss: 0.1515\n",
            "Epoch [59/100], Loss: 0.1674, Val Loss: 0.1210\n",
            "Epoch [60/100], Loss: 0.0644, Val Loss: 0.1145\n",
            "Epoch [61/100], Loss: 0.0887, Val Loss: 0.1199\n",
            "Epoch [62/100], Loss: 0.1433, Val Loss: 0.1153\n",
            "Epoch [63/100], Loss: 0.4717, Val Loss: 0.1109\n",
            "Epoch [64/100], Loss: 0.2364, Val Loss: 0.1058\n",
            "Epoch [65/100], Loss: 0.2235, Val Loss: 0.1029\n",
            "Epoch [66/100], Loss: 0.0401, Val Loss: 0.1062\n",
            "Epoch [67/100], Loss: 0.0875, Val Loss: 0.1006\n",
            "Epoch [68/100], Loss: 0.1554, Val Loss: 0.1081\n",
            "Epoch [69/100], Loss: 0.1142, Val Loss: 0.1056\n",
            "Epoch [70/100], Loss: 0.0124, Val Loss: 0.0947\n",
            "Epoch [71/100], Loss: 0.1830, Val Loss: 0.0938\n",
            "Epoch [72/100], Loss: 0.0642, Val Loss: 0.1154\n",
            "Epoch [73/100], Loss: 0.0386, Val Loss: 0.0901\n",
            "Epoch [74/100], Loss: 0.0325, Val Loss: 0.0870\n",
            "Epoch [75/100], Loss: 0.0094, Val Loss: 0.0925\n",
            "Epoch [76/100], Loss: 0.1331, Val Loss: 0.0851\n",
            "Epoch [77/100], Loss: 0.0827, Val Loss: 0.0911\n",
            "Epoch [78/100], Loss: 0.1114, Val Loss: 0.0844\n",
            "Epoch [79/100], Loss: 0.1731, Val Loss: 0.0799\n",
            "Epoch [80/100], Loss: 0.0047, Val Loss: 0.0924\n",
            "Epoch [81/100], Loss: 0.0067, Val Loss: 0.0797\n",
            "Epoch [82/100], Loss: 0.1927, Val Loss: 0.0766\n",
            "Epoch [83/100], Loss: 0.1962, Val Loss: 0.0759\n",
            "Epoch [84/100], Loss: 0.4166, Val Loss: 0.0954\n",
            "Epoch [85/100], Loss: 0.0173, Val Loss: 0.0749\n",
            "Epoch [86/100], Loss: 0.0575, Val Loss: 0.0736\n",
            "Epoch [87/100], Loss: 0.0888, Val Loss: 0.0873\n",
            "Epoch [88/100], Loss: 0.2768, Val Loss: 0.0721\n",
            "Epoch [89/100], Loss: 0.2207, Val Loss: 0.0730\n",
            "Epoch [90/100], Loss: 0.0111, Val Loss: 0.0895\n",
            "Epoch [91/100], Loss: 0.2457, Val Loss: 0.0709\n",
            "Epoch [92/100], Loss: 0.2294, Val Loss: 0.0686\n",
            "Epoch [93/100], Loss: 0.1329, Val Loss: 0.0774\n",
            "Epoch [94/100], Loss: 0.3855, Val Loss: 0.0662\n",
            "Epoch [95/100], Loss: 0.0334, Val Loss: 0.1008\n",
            "Epoch [96/100], Loss: 0.1861, Val Loss: 0.0653\n",
            "Epoch [97/100], Loss: 0.0769, Val Loss: 0.0653\n",
            "Epoch [98/100], Loss: 0.0946, Val Loss: 0.0654\n",
            "Epoch [99/100], Loss: 0.0514, Val Loss: 0.0698\n",
            "Epoch [100/100], Loss: 0.1241, Val Loss: 0.0625\n",
            "Test Loss: 0.2502559423446655\n",
            "Test Accuracy: 0.8999999761581421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "iris_df = pd.read_csv(\"/content/drive/MyDrive/assignment28/Iris.csv\")\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris_df[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
        "y = iris_df[\"Species\"].replace({\"Iris-versicolor\": 0, \"Iris-virginica\": 1, \"Iris-setosa\": 2}).values\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "X_test, y_test = X_temp, y_temp\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Neural network model\n",
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiClassClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(X_train.shape[1], 50)\n",
        "        self.fc2 = nn.Linear(50, 100)\n",
        "        self.fc3 = nn.Linear(100, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = MultiClassClassifier()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            outputs = model(X_batch)\n",
        "            val_loss += criterion(outputs, y_batch).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test).item()\n",
        "    test_acc = (test_outputs.argmax(dim=1) == y_test).float().mean().item()\n",
        "    print(\"Test Loss:\", test_loss)\n",
        "    print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ-UZqcMi4i5",
        "outputId": "87300b09-645c-4a89-d082-d8b1995c94fd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.9794, Val Loss: 0.9435\n",
            "Epoch [2/100], Loss: 0.8450, Val Loss: 0.8540\n",
            "Epoch [3/100], Loss: 0.7413, Val Loss: 0.7465\n",
            "Epoch [4/100], Loss: 0.6907, Val Loss: 0.6526\n",
            "Epoch [5/100], Loss: 0.6312, Val Loss: 0.5606\n",
            "Epoch [6/100], Loss: 0.6506, Val Loss: 0.4839\n",
            "Epoch [7/100], Loss: 0.5533, Val Loss: 0.4320\n",
            "Epoch [8/100], Loss: 0.4611, Val Loss: 0.4129\n",
            "Epoch [9/100], Loss: 0.4924, Val Loss: 0.3685\n",
            "Epoch [10/100], Loss: 0.2732, Val Loss: 0.3519\n",
            "Epoch [11/100], Loss: 0.2964, Val Loss: 0.3346\n",
            "Epoch [12/100], Loss: 0.2871, Val Loss: 0.3172\n",
            "Epoch [13/100], Loss: 0.2046, Val Loss: 0.3007\n",
            "Epoch [14/100], Loss: 0.1727, Val Loss: 0.2800\n",
            "Epoch [15/100], Loss: 0.2478, Val Loss: 0.2649\n",
            "Epoch [16/100], Loss: 0.2523, Val Loss: 0.2719\n",
            "Epoch [17/100], Loss: 0.1846, Val Loss: 0.2577\n",
            "Epoch [18/100], Loss: 0.1275, Val Loss: 0.2407\n",
            "Epoch [19/100], Loss: 0.1348, Val Loss: 0.2409\n",
            "Epoch [20/100], Loss: 0.1827, Val Loss: 0.2265\n",
            "Epoch [21/100], Loss: 0.1359, Val Loss: 0.2227\n",
            "Epoch [22/100], Loss: 0.1074, Val Loss: 0.2206\n",
            "Epoch [23/100], Loss: 0.2184, Val Loss: 0.2108\n",
            "Epoch [24/100], Loss: 0.0597, Val Loss: 0.2243\n",
            "Epoch [25/100], Loss: 0.1489, Val Loss: 0.2088\n",
            "Epoch [26/100], Loss: 0.0957, Val Loss: 0.2077\n",
            "Epoch [27/100], Loss: 0.1385, Val Loss: 0.1988\n",
            "Epoch [28/100], Loss: 0.0455, Val Loss: 0.1982\n",
            "Epoch [29/100], Loss: 0.1517, Val Loss: 0.1975\n",
            "Epoch [30/100], Loss: 0.0192, Val Loss: 0.1987\n",
            "Epoch [31/100], Loss: 0.0332, Val Loss: 0.1954\n",
            "Epoch [32/100], Loss: 0.2238, Val Loss: 0.1915\n",
            "Epoch [33/100], Loss: 0.0314, Val Loss: 0.1961\n",
            "Epoch [34/100], Loss: 0.0789, Val Loss: 0.1888\n",
            "Epoch [35/100], Loss: 0.0233, Val Loss: 0.1957\n",
            "Epoch [36/100], Loss: 0.1666, Val Loss: 0.1914\n",
            "Epoch [37/100], Loss: 0.0925, Val Loss: 0.1907\n",
            "Epoch [38/100], Loss: 0.1027, Val Loss: 0.1869\n",
            "Epoch [39/100], Loss: 0.0337, Val Loss: 0.1837\n",
            "Epoch [40/100], Loss: 0.0601, Val Loss: 0.1850\n",
            "Epoch [41/100], Loss: 0.0674, Val Loss: 0.1823\n",
            "Epoch [42/100], Loss: 0.0292, Val Loss: 0.1817\n",
            "Epoch [43/100], Loss: 0.2265, Val Loss: 0.1815\n",
            "Epoch [44/100], Loss: 0.1198, Val Loss: 0.1821\n",
            "Epoch [45/100], Loss: 0.2860, Val Loss: 0.1858\n",
            "Epoch [46/100], Loss: 0.0024, Val Loss: 0.1814\n",
            "Epoch [47/100], Loss: 0.1063, Val Loss: 0.1821\n",
            "Epoch [48/100], Loss: 0.2048, Val Loss: 0.1821\n",
            "Epoch [49/100], Loss: 0.1706, Val Loss: 0.1927\n",
            "Epoch [50/100], Loss: 0.0076, Val Loss: 0.1854\n",
            "Epoch [51/100], Loss: 0.2550, Val Loss: 0.1801\n",
            "Epoch [52/100], Loss: 0.0180, Val Loss: 0.2048\n",
            "Epoch [53/100], Loss: 0.0432, Val Loss: 0.1880\n",
            "Epoch [54/100], Loss: 0.0162, Val Loss: 0.1786\n",
            "Epoch [55/100], Loss: 0.0602, Val Loss: 0.1768\n",
            "Epoch [56/100], Loss: 0.0088, Val Loss: 0.1767\n",
            "Epoch [57/100], Loss: 0.0783, Val Loss: 0.1787\n",
            "Epoch [58/100], Loss: 0.2971, Val Loss: 0.1996\n",
            "Epoch [59/100], Loss: 0.0053, Val Loss: 0.2094\n",
            "Epoch [60/100], Loss: 0.0129, Val Loss: 0.1871\n",
            "Epoch [61/100], Loss: 0.0005, Val Loss: 0.1787\n",
            "Epoch [62/100], Loss: 0.0425, Val Loss: 0.1760\n",
            "Epoch [63/100], Loss: 0.0119, Val Loss: 0.1754\n",
            "Epoch [64/100], Loss: 0.0693, Val Loss: 0.1757\n",
            "Epoch [65/100], Loss: 0.0058, Val Loss: 0.1754\n",
            "Epoch [66/100], Loss: 0.0128, Val Loss: 0.1762\n",
            "Epoch [67/100], Loss: 0.1264, Val Loss: 0.1760\n",
            "Epoch [68/100], Loss: 0.0072, Val Loss: 0.1852\n",
            "Epoch [69/100], Loss: 0.0249, Val Loss: 0.1841\n",
            "Epoch [70/100], Loss: 0.0017, Val Loss: 0.1840\n",
            "Epoch [71/100], Loss: 0.0430, Val Loss: 0.1753\n",
            "Epoch [72/100], Loss: 0.0229, Val Loss: 0.1749\n",
            "Epoch [73/100], Loss: 0.2194, Val Loss: 0.1791\n",
            "Epoch [74/100], Loss: 0.0440, Val Loss: 0.1821\n",
            "Epoch [75/100], Loss: 0.0008, Val Loss: 0.1885\n",
            "Epoch [76/100], Loss: 0.0090, Val Loss: 0.1735\n",
            "Epoch [77/100], Loss: 0.0653, Val Loss: 0.1797\n",
            "Epoch [78/100], Loss: 0.0047, Val Loss: 0.1786\n",
            "Epoch [79/100], Loss: 0.0049, Val Loss: 0.1758\n",
            "Epoch [80/100], Loss: 0.0507, Val Loss: 0.1768\n",
            "Epoch [81/100], Loss: 0.0696, Val Loss: 0.1918\n",
            "Epoch [82/100], Loss: 0.0020, Val Loss: 0.1844\n",
            "Epoch [83/100], Loss: 0.0063, Val Loss: 0.1795\n",
            "Epoch [84/100], Loss: 0.0042, Val Loss: 0.1741\n",
            "Epoch [85/100], Loss: 0.0078, Val Loss: 0.1880\n",
            "Epoch [86/100], Loss: 0.1199, Val Loss: 0.1752\n",
            "Epoch [87/100], Loss: 0.0013, Val Loss: 0.1770\n",
            "Epoch [88/100], Loss: 0.0170, Val Loss: 0.1757\n",
            "Epoch [89/100], Loss: 0.0052, Val Loss: 0.1847\n",
            "Epoch [90/100], Loss: 0.0007, Val Loss: 0.1750\n",
            "Epoch [91/100], Loss: 0.0048, Val Loss: 0.1946\n",
            "Epoch [92/100], Loss: 0.0058, Val Loss: 0.1805\n",
            "Epoch [93/100], Loss: 0.1399, Val Loss: 0.1745\n",
            "Epoch [94/100], Loss: 0.0047, Val Loss: 0.1873\n",
            "Epoch [95/100], Loss: 0.0108, Val Loss: 0.1797\n",
            "Epoch [96/100], Loss: 0.0029, Val Loss: 0.1915\n",
            "Epoch [97/100], Loss: 0.0224, Val Loss: 0.1748\n",
            "Epoch [98/100], Loss: 0.0779, Val Loss: 0.1863\n",
            "Epoch [99/100], Loss: 0.0007, Val Loss: 0.1761\n",
            "Epoch [100/100], Loss: 0.0018, Val Loss: 0.2033\n",
            "Test Loss: 0.01851832866668701\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "house_df = pd.read_csv(\"/content/drive/MyDrive/assignment28/train.csv\")\n",
        "\n",
        "# Extract features and labels\n",
        "X = house_df[[\"GrLivArea\", \"YearBuilt\"]].values\n",
        "y = house_df[\"SalePrice\"].values\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "X_test, y_test = X_temp, y_temp\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Neural network model\n",
        "class HousePriceRegressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HousePriceRegressor, self).__init__()\n",
        "        self.fc1 = nn.Linear(X_train.shape[1], 50)\n",
        "        self.fc2 = nn.Linear(50, 100)\n",
        "        self.fc3 = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = HousePriceRegressor()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            outputs = model(X_batch)\n",
        "            val_loss += criterion(outputs, y_batch).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test).item()\n",
        "    print(\"Test MSE:\", test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-SYb_2IjYWB",
        "outputId": "128fc10b-b10f-4d7b-e88d-8874dd85d6f5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 21776162816.0000, Val Loss: 30235884202.6667\n",
            "Epoch [2/100], Loss: 4147607040.0000, Val Loss: 6708678672.0000\n",
            "Epoch [3/100], Loss: 2273930752.0000, Val Loss: 3290898984.0000\n",
            "Epoch [4/100], Loss: 1119881344.0000, Val Loss: 3191094834.6667\n",
            "Epoch [5/100], Loss: 874884864.0000, Val Loss: 2987294256.0000\n",
            "Epoch [6/100], Loss: 2969547008.0000, Val Loss: 2875024309.3333\n",
            "Epoch [7/100], Loss: 12514043904.0000, Val Loss: 2759914698.6667\n",
            "Epoch [8/100], Loss: 1422142976.0000, Val Loss: 2744521570.6667\n",
            "Epoch [9/100], Loss: 16364515328.0000, Val Loss: 2640905077.3333\n",
            "Epoch [10/100], Loss: 710188800.0000, Val Loss: 2651531728.0000\n",
            "Epoch [11/100], Loss: 5516817920.0000, Val Loss: 2516788186.6667\n",
            "Epoch [12/100], Loss: 9341265920.0000, Val Loss: 2459893485.3333\n",
            "Epoch [13/100], Loss: 1620108672.0000, Val Loss: 2430738194.6667\n",
            "Epoch [14/100], Loss: 826764288.0000, Val Loss: 2455953173.3333\n",
            "Epoch [15/100], Loss: 684907968.0000, Val Loss: 2409616138.6667\n",
            "Epoch [16/100], Loss: 703214016.0000, Val Loss: 2403808300.0000\n",
            "Epoch [17/100], Loss: 12742278144.0000, Val Loss: 2440380228.0000\n",
            "Epoch [18/100], Loss: 1457999232.0000, Val Loss: 2421097973.3333\n",
            "Epoch [19/100], Loss: 545050752.0000, Val Loss: 2446033429.3333\n",
            "Epoch [20/100], Loss: 2929474560.0000, Val Loss: 2473803766.6667\n",
            "Epoch [21/100], Loss: 839959680.0000, Val Loss: 2469110332.0000\n",
            "Epoch [22/100], Loss: 3270661888.0000, Val Loss: 2406566013.3333\n",
            "Epoch [23/100], Loss: 1721346432.0000, Val Loss: 2408382081.3333\n",
            "Epoch [24/100], Loss: 2732421376.0000, Val Loss: 2445088174.6667\n",
            "Epoch [25/100], Loss: 1767592320.0000, Val Loss: 2453651837.3333\n",
            "Epoch [26/100], Loss: 2897980160.0000, Val Loss: 2417879654.6667\n",
            "Epoch [27/100], Loss: 238700192.0000, Val Loss: 2430741286.6667\n",
            "Epoch [28/100], Loss: 1271530496.0000, Val Loss: 2445891306.6667\n",
            "Epoch [29/100], Loss: 774133696.0000, Val Loss: 2385713838.6667\n",
            "Epoch [30/100], Loss: 841825664.0000, Val Loss: 2472166353.3333\n",
            "Epoch [31/100], Loss: 1144388608.0000, Val Loss: 2471242157.3333\n",
            "Epoch [32/100], Loss: 340822944.0000, Val Loss: 2397107088.0000\n",
            "Epoch [33/100], Loss: 214082224.0000, Val Loss: 2464170708.0000\n",
            "Epoch [34/100], Loss: 1727281280.0000, Val Loss: 2420223736.0000\n",
            "Epoch [35/100], Loss: 839923648.0000, Val Loss: 2389467416.0000\n",
            "Epoch [36/100], Loss: 1127193728.0000, Val Loss: 2442064461.3333\n",
            "Epoch [37/100], Loss: 468555840.0000, Val Loss: 2400779226.6667\n",
            "Epoch [38/100], Loss: 10818921472.0000, Val Loss: 2453972617.3333\n",
            "Epoch [39/100], Loss: 1527475584.0000, Val Loss: 2401670098.6667\n",
            "Epoch [40/100], Loss: 865792704.0000, Val Loss: 2393786441.3333\n",
            "Epoch [41/100], Loss: 354146432.0000, Val Loss: 2426614454.6667\n",
            "Epoch [42/100], Loss: 984322304.0000, Val Loss: 2440901734.6667\n",
            "Epoch [43/100], Loss: 3846436352.0000, Val Loss: 2411453034.6667\n",
            "Epoch [44/100], Loss: 14247507968.0000, Val Loss: 2446767041.3333\n",
            "Epoch [45/100], Loss: 2079330688.0000, Val Loss: 2413910113.3333\n",
            "Epoch [46/100], Loss: 327076640.0000, Val Loss: 2463315334.6667\n",
            "Epoch [47/100], Loss: 1642097024.0000, Val Loss: 2389648593.3333\n",
            "Epoch [48/100], Loss: 949859328.0000, Val Loss: 2423784249.3333\n",
            "Epoch [49/100], Loss: 1169432832.0000, Val Loss: 2455009269.3333\n",
            "Epoch [50/100], Loss: 6696962048.0000, Val Loss: 2419962441.3333\n",
            "Epoch [51/100], Loss: 4048187136.0000, Val Loss: 2455389693.3333\n",
            "Epoch [52/100], Loss: 9750220800.0000, Val Loss: 2414270070.6667\n",
            "Epoch [53/100], Loss: 2645748480.0000, Val Loss: 2390489082.6667\n",
            "Epoch [54/100], Loss: 8254102016.0000, Val Loss: 2427960129.3333\n",
            "Epoch [55/100], Loss: 4639415808.0000, Val Loss: 2423066857.3333\n",
            "Epoch [56/100], Loss: 2554023680.0000, Val Loss: 2470206608.0000\n",
            "Epoch [57/100], Loss: 4835620864.0000, Val Loss: 2394465776.0000\n",
            "Epoch [58/100], Loss: 7239734272.0000, Val Loss: 2408541534.6667\n",
            "Epoch [59/100], Loss: 1664011392.0000, Val Loss: 2496666070.6667\n",
            "Epoch [60/100], Loss: 1670509696.0000, Val Loss: 2389538788.0000\n",
            "Epoch [61/100], Loss: 2154700032.0000, Val Loss: 2390973142.6667\n",
            "Epoch [62/100], Loss: 139150656.0000, Val Loss: 2404681536.0000\n",
            "Epoch [63/100], Loss: 414404416.0000, Val Loss: 2456104788.0000\n",
            "Epoch [64/100], Loss: 2578911488.0000, Val Loss: 2435501620.0000\n",
            "Epoch [65/100], Loss: 11246811136.0000, Val Loss: 2390104682.6667\n",
            "Epoch [66/100], Loss: 1039500672.0000, Val Loss: 2471969208.0000\n",
            "Epoch [67/100], Loss: 8611219456.0000, Val Loss: 2420950585.3333\n",
            "Epoch [68/100], Loss: 265590080.0000, Val Loss: 2399135113.3333\n",
            "Epoch [69/100], Loss: 2353574400.0000, Val Loss: 2393490606.6667\n",
            "Epoch [70/100], Loss: 90532952.0000, Val Loss: 2407913500.0000\n",
            "Epoch [71/100], Loss: 385579296.0000, Val Loss: 2457820716.0000\n",
            "Epoch [72/100], Loss: 1350489472.0000, Val Loss: 2550155957.3333\n",
            "Epoch [73/100], Loss: 5537769984.0000, Val Loss: 2395527141.3333\n",
            "Epoch [74/100], Loss: 287346240.0000, Val Loss: 2408999794.6667\n",
            "Epoch [75/100], Loss: 2111738752.0000, Val Loss: 2401102602.6667\n",
            "Epoch [76/100], Loss: 3388246016.0000, Val Loss: 2407727582.6667\n",
            "Epoch [77/100], Loss: 2581463040.0000, Val Loss: 2420968906.6667\n",
            "Epoch [78/100], Loss: 791223936.0000, Val Loss: 2401033978.6667\n",
            "Epoch [79/100], Loss: 6886350848.0000, Val Loss: 2406086061.3333\n",
            "Epoch [80/100], Loss: 3757148416.0000, Val Loss: 2450062185.3333\n",
            "Epoch [81/100], Loss: 1507964928.0000, Val Loss: 2409208542.6667\n",
            "Epoch [82/100], Loss: 3710902016.0000, Val Loss: 2594783112.0000\n",
            "Epoch [83/100], Loss: 5327493120.0000, Val Loss: 2390956396.0000\n",
            "Epoch [84/100], Loss: 1055402560.0000, Val Loss: 2438368265.3333\n",
            "Epoch [85/100], Loss: 2041634560.0000, Val Loss: 2423091158.6667\n",
            "Epoch [86/100], Loss: 1810688384.0000, Val Loss: 2422881400.0000\n",
            "Epoch [87/100], Loss: 4160285184.0000, Val Loss: 2399555258.6667\n",
            "Epoch [88/100], Loss: 1592721280.0000, Val Loss: 2450680516.0000\n",
            "Epoch [89/100], Loss: 935314752.0000, Val Loss: 2407332472.0000\n",
            "Epoch [90/100], Loss: 755548992.0000, Val Loss: 2401936844.0000\n",
            "Epoch [91/100], Loss: 841461568.0000, Val Loss: 2401635124.0000\n",
            "Epoch [92/100], Loss: 1564244992.0000, Val Loss: 2406000285.3333\n",
            "Epoch [93/100], Loss: 601480320.0000, Val Loss: 2428869762.6667\n",
            "Epoch [94/100], Loss: 3908985344.0000, Val Loss: 2404098970.6667\n",
            "Epoch [95/100], Loss: 2308754688.0000, Val Loss: 2424349742.6667\n",
            "Epoch [96/100], Loss: 413152640.0000, Val Loss: 2512021378.6667\n",
            "Epoch [97/100], Loss: 3570068992.0000, Val Loss: 2397821216.0000\n",
            "Epoch [98/100], Loss: 604251456.0000, Val Loss: 2392588356.0000\n",
            "Epoch [99/100], Loss: 301733056.0000, Val Loss: 2406243824.0000\n",
            "Epoch [100/100], Loss: 2478124288.0000, Val Loss: 2557693281.3333\n",
            "Test MSE: 3848152832.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize data\n",
        "X_train = X_train.reshape(-1, 784).astype('float32') / 255\n",
        "X_test = X_test.reshape(-1, 784).astype('float32') / 255\n",
        "\n",
        "# One-hot encode labels\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_one_hot = enc.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train, y_train_one_hot = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train_one_hot, dtype=torch.float32)\n",
        "X_test, y_test_one_hot = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test_one_hot, dtype=torch.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = TensorDataset(X_train, y_train_one_hot)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Neural network model\n",
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(X_train.shape[1], 50)\n",
        "        self.fc2 = nn.Linear(50, 100)\n",
        "        self.fc3 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.softmax(self.fc3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "model = MNISTClassifier()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test_one_hot).item()\n",
        "    test_acc = (test_outputs.argmax(dim=1) == y_test_one_hot.argmax(dim=1)).float().mean().item()\n",
        "    print(\"Test Loss:\", test_loss)\n",
        "    print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzTJF3zgkKad",
        "outputId": "cacf5a3e-4c57-4b1b-a902-658abb984ea2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.4614\n",
            "Epoch [2/10], Loss: 1.5186\n",
            "Epoch [3/10], Loss: 1.4612\n",
            "Epoch [4/10], Loss: 1.4680\n",
            "Epoch [5/10], Loss: 1.4612\n",
            "Epoch [6/10], Loss: 1.4612\n",
            "Epoch [7/10], Loss: 1.4631\n",
            "Epoch [8/10], Loss: 1.4612\n",
            "Epoch [9/10], Loss: 1.4612\n",
            "Epoch [10/10], Loss: 1.4612\n",
            "Test Loss: 1.4968708753585815\n",
            "Test Accuracy: 0.9643999934196472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Eight: (Advance assignment) Comparison of frameworks\n",
        "When comparing TensorFlow and Keras, there are several aspects to consider:\n",
        "\n",
        "Here is a summary of the differences between Tensorflow, Keras, and PyTorch:\n",
        "\n",
        "Calculation Speed: All three frameworks have comparable calculation speeds, with each excelling in specific scenarios based on model architecture and hardware utilization.\n",
        "\n",
        "Number of Lines of Code and Readability: Keras stands out for its conciseness and readability due to its high-level API, making it beginner-friendly. PyTorch offers a balance between readability and flexibility, while TensorFlow, being more low-level, might require more code for similar tasks.\n",
        "\n",
        "Functions Provided: TensorFlow boasts a comprehensive suite of functions, catering to both research and production needs. PyTorch provides a dynamic computation graph, allowing for greater flexibility during model development. Keras, with its streamlined API, focuses on ease of use for common deep learning tasks.\n",
        "\n",
        "In summary, Keras is the go-to choice for beginners and rapid prototyping due to its simplicity and ease of use. PyTorch, with its balance of flexibility and readability, is favored by researchers for its dynamic computation graphs. TensorFlow, with its extensive functionalities, caters to both research and production, offering fine-grained control and scalability for experienced users."
      ],
      "metadata": {
        "id": "Dd9qUONXIpED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TQnq2e35Is5l"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}